{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup---imports\" data-toc-modified-id=\"Setup---imports-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Setup - imports</a></span></li><li><span><a href=\"#Setup---output\" data-toc-modified-id=\"Setup---output-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Setup - output</a></span><ul class=\"toc-item\"><li><span><a href=\"#function-make_path\" data-toc-modified-id=\"function-make_path-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>function make_path</a></span></li><li><span><a href=\"#set-up-output-configuration\" data-toc-modified-id=\"set-up-output-configuration-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>set up output configuration</a></span></li></ul></li><li><span><a href=\"#Setup---Functions\" data-toc-modified-id=\"Setup---Functions-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setup - Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#function-plot_precision_recall_n\" data-toc-modified-id=\"function-plot_precision_recall_n-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>function plot_precision_recall_n</a></span></li><li><span><a href=\"#function-threshold_at_k\" data-toc-modified-id=\"function-threshold_at_k-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>function threshold_at_k</a></span></li><li><span><a href=\"#function-precision_at_k\" data-toc-modified-id=\"function-precision_at_k-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>function precision_at_k</a></span></li><li><span><a href=\"#function-recall_at_k\" data-toc-modified-id=\"function-recall_at_k-1.3.4\"><span class=\"toc-item-num\">1.3.4&nbsp;&nbsp;</span>function recall_at_k</a></span></li><li><span><a href=\"#function-accuracy_at_k\" data-toc-modified-id=\"function-accuracy_at_k-1.3.5\"><span class=\"toc-item-num\">1.3.5&nbsp;&nbsp;</span>function accuracy_at_k</a></span></li><li><span><a href=\"#function-precision_recall_f1\" data-toc-modified-id=\"function-precision_recall_f1-1.3.6\"><span class=\"toc-item-num\">1.3.6&nbsp;&nbsp;</span>function precision_recall_f1</a></span></li></ul></li></ul></li><li><span><a href=\"#class-CitationCodingEvaluation\" data-toc-modified-id=\"class-CitationCodingEvaluation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>class CitationCodingEvaluation</a></span></li><li><span><a href=\"#Load-JSON-files\" data-toc-modified-id=\"Load-JSON-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load JSON files</a></span></li><li><span><a href=\"#Evaluate-all-publications\" data-toc-modified-id=\"Evaluate-all-publications-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Evaluate all publications</a></span><ul class=\"toc-item\"><li><span><a href=\"#Process-JSON\" data-toc-modified-id=\"Process-JSON-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Process JSON</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-publications?\" data-toc-modified-id=\"Filter-publications?-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Filter publications?</a></span><ul class=\"toc-item\"><li><span><a href=\"#django-init\" data-toc-modified-id=\"django-init-4.1.1.1\"><span class=\"toc-item-num\">4.1.1.1&nbsp;&nbsp;</span>django init</a></span></li><li><span><a href=\"#get-IDs-of-included-publications\" data-toc-modified-id=\"get-IDs-of-included-publications-4.1.1.2\"><span class=\"toc-item-num\">4.1.1.2&nbsp;&nbsp;</span>get IDs of included publications</a></span></li></ul></li><li><span><a href=\"#process-JSON-files\" data-toc-modified-id=\"process-JSON-files-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>process JSON files</a></span></li></ul></li><li><span><a href=\"#precision,-recall,-and-accuracy\" data-toc-modified-id=\"precision,-recall,-and-accuracy-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>precision, recall, and accuracy</a></span></li><li><span><a href=\"#precision,-recall,-and-accuracy-per-publication\" data-toc-modified-id=\"precision,-recall,-and-accuracy-per-publication-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>precision, recall, and accuracy per publication</a></span><ul class=\"toc-item\"><li><span><a href=\"#false-positives-(FP)\" data-toc-modified-id=\"false-positives-(FP)-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>false positives (FP)</a></span></li><li><span><a href=\"#false-negatives-(FN)\" data-toc-modified-id=\"false-negatives-(FN)-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>false negatives (FN)</a></span></li><li><span><a href=\"#output-all-publication-citation-pairs\" data-toc-modified-id=\"output-all-publication-citation-pairs-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;</span>output all publication-citation pairs</a></span></li></ul></li><li><span><a href=\"#graph-precision-and-recall-at-n\" data-toc-modified-id=\"graph-precision-and-recall-at-n-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>graph precision and recall at n</a></span></li><li><span><a href=\"#output-results-to-file\" data-toc-modified-id=\"output-results-to-file-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>output results to file</a></span></li></ul></li><li><span><a href=\"#Evaluate-only-publications-with-citations\" data-toc-modified-id=\"Evaluate-only-publications-with-citations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluate only publications with citations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Process-JSON\" data-toc-modified-id=\"Process-JSON-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Process JSON</a></span><ul class=\"toc-item\"><li><span><a href=\"#Filter-publications?\" data-toc-modified-id=\"Filter-publications?-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Filter publications?</a></span><ul class=\"toc-item\"><li><span><a href=\"#django-init\" data-toc-modified-id=\"django-init-5.1.1.1\"><span class=\"toc-item-num\">5.1.1.1&nbsp;&nbsp;</span>django init</a></span></li><li><span><a href=\"#get-IDs-of-included-publications\" data-toc-modified-id=\"get-IDs-of-included-publications-5.1.1.2\"><span class=\"toc-item-num\">5.1.1.2&nbsp;&nbsp;</span>get IDs of included publications</a></span></li></ul></li><li><span><a href=\"#process-JSON-files\" data-toc-modified-id=\"process-JSON-files-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>process JSON files</a></span></li></ul></li><li><span><a href=\"#precision,-recall,-and-accuracy\" data-toc-modified-id=\"precision,-recall,-and-accuracy-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>precision, recall, and accuracy</a></span></li><li><span><a href=\"#precision,-recall,-and-accuracy-per-publication\" data-toc-modified-id=\"precision,-recall,-and-accuracy-per-publication-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>precision, recall, and accuracy per publication</a></span><ul class=\"toc-item\"><li><span><a href=\"#false-positives-(FP)\" data-toc-modified-id=\"false-positives-(FP)-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>false positives (FP)</a></span></li><li><span><a href=\"#false-negatives-(FN)\" data-toc-modified-id=\"false-negatives-(FN)-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>false negatives (FN)</a></span></li><li><span><a href=\"#output-all-publication-citation-pairs\" data-toc-modified-id=\"output-all-publication-citation-pairs-5.3.3\"><span class=\"toc-item-num\">5.3.3&nbsp;&nbsp;</span>output all publication-citation pairs</a></span></li></ul></li><li><span><a href=\"#graph-precision-and-recall-at-n\" data-toc-modified-id=\"graph-precision-and-recall-at-n-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>graph precision and recall at n</a></span></li><li><span><a href=\"#output-results-to-file\" data-toc-modified-id=\"output-results-to-file-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>output results to file</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "debug_flag = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - imports\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "import os\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - output\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function make_path\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# function to create a path if it does not exist\n",
    "def make_path(path_to_make):\n",
    "    if not os.path.exists(path_to_make):\n",
    "        print(\"Creating path {}\".format(path_to_make))\n",
    "        os.makedirs(path_to_make)\n",
    "    else:\n",
    "        print(\"{} already exists\".format(path_to_make))\n",
    "\n",
    "    return path_to_make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up output configuration\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TEAM_NAME = \"rcc-14\"\n",
    "SUBMISSION_FOLDER = \"2019.01.24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "debug_flag = False\n",
    "\n",
    "# file name variables\n",
    "file_name_prefix = \"{}-{}-\".format( TEAM_NAME, SUBMISSION_FOLDER )\n",
    "file_name_suffix = \"\"\n",
    "\n",
    "# output_to_file flag\n",
    "output_to_file = True\n",
    "line_list = None\n",
    "output_string = None\n",
    "#output_folder_path = \"/data/output\"\n",
    "#output_folder_path = \".\"\n",
    "output_folder_path = \"/work/evaluate/{}/{}/evaluate/holdout\".format( TEAM_NAME, SUBMISSION_FOLDER )\n",
    "make_path( output_folder_path )\n",
    "results_file_path = \"{}/{}evaluation_results{}.txt\".format( output_folder_path, file_name_prefix, file_name_suffix )\n",
    "precision_recall_graph_path = \"{}/{}precision_recall_graph{}.pdf\".format( output_folder_path, file_name_prefix, file_name_suffix )\n",
    "\n",
    "# if we are outputting to file, start line list.\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # put a list in line_list\n",
    "    line_list = []\n",
    "    \n",
    "#-- END init line list --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Functions\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function plot_precision_recall_n\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name, output_path_IN = None ):\n",
    "\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    \n",
    "    # imports\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "    # return reference\n",
    "    details_OUT = {}\n",
    "    \n",
    "    # declare variables\n",
    "    me = \"plot_precision_recall_n\"\n",
    "    y_score = None\n",
    "    precision_curve = None\n",
    "    recall_curve = None\n",
    "    pr_thresholds = None\n",
    "    num_above_thresh = None\n",
    "    pct_above_thresh = None\n",
    "    pct_above_per_thresh = None\n",
    "    current_score = None\n",
    "    above_threshold_list = None\n",
    "    above_threshold_count = -1\n",
    "    fig = None\n",
    "    ax1 = None\n",
    "    ax2 = None\n",
    "    \n",
    "    # store the raw scores in y_score\n",
    "    y_score = y_prob\n",
    "    \n",
    "    # calculate precision-recall curve\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n",
    "    # Returns:\n",
    "    # - precision_curve - Precison values such that element i is the precision of predictions where cutoff is score >= thresholds[ i ] and the last element is 1.\n",
    "    # - recall_curve - Recall values such that element i is the recall of predictions where cutoff is score >= thresholds[ i ] and the last element is 0.\n",
    "    # - pr_thresholds - Increasing thresholds on the decision function used to decide 1 or 0, used to calculate precision and recall (looks like it is the set of unique values in the predicted value set).\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve( y_true, y_score )\n",
    "    \n",
    "    # get all but the last precision score (1).\n",
    "    precision_curve = precision_curve[ : -1 ]\n",
    "    # print( \"precision_curve: {}\".format( precision_curve ) )\n",
    "    \n",
    "    # get all but the last recall score (0).\n",
    "    recall_curve = recall_curve[ : -1 ]\n",
    "    # print( \"recall_curve: {}\".format( recall_curve ) )\n",
    "    \n",
    "    # store details\n",
    "    details_OUT[ \"precision\" ] = precision_curve\n",
    "    details_OUT[ \"recall\" ] = recall_curve\n",
    "    details_OUT[ \"threshold\" ] = pr_thresholds\n",
    "    \n",
    "    # init loop over thresholds\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    \n",
    "    # loop over thresholds\n",
    "    for value in pr_thresholds:\n",
    "        \n",
    "        # at each threshold, calculate the percent of rows above the threshold.\n",
    "        above_threshold_list = []\n",
    "        above_threshold_count = -1\n",
    "        for current_score in y_score:\n",
    "            \n",
    "            # is it at or above threshold?\n",
    "            if ( current_score >= value ):\n",
    "                \n",
    "                # it is either at or above threshold - add to list.\n",
    "                above_threshold_list.append( current_score )\n",
    "                \n",
    "            #-- END check to see if at or above threshold? --#\n",
    "                \n",
    "        #-- END loop over scores. --#\n",
    "\n",
    "        # how many above threshold?\n",
    "        #num_above_thresh = len(y_score[y_score>=value])\n",
    "        above_threshold_count = len( above_threshold_list )\n",
    "        num_above_thresh = above_threshold_count\n",
    "        \n",
    "        # percent above threshold\n",
    "        pct_above_thresh = num_above_thresh / float( number_scored )\n",
    "        \n",
    "        # add to list.\n",
    "        pct_above_per_thresh.append( pct_above_thresh )\n",
    "        \n",
    "    #-- END loop over thresholds --#\n",
    "\n",
    "    details_OUT[ \"percent_above\" ] = pct_above_per_thresh\n",
    "    \n",
    "    # convert to numpy array\n",
    "    pct_above_per_thresh = numpy.array(pct_above_per_thresh)\n",
    "\n",
    "    # init matplotlib\n",
    "    matplotlib.pyplot.clf()\n",
    "    fig, ax1 = matplotlib.pyplot.subplots()\n",
    "    \n",
    "    # plot % above threshold line\n",
    "    ax1.plot( pr_thresholds, pct_above_per_thresh, 'y')\n",
    "    ax1.set_xlabel('threshold values')\n",
    "    matplotlib.pyplot.xticks( [ 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ] )\n",
    "    ax1.set_ylabel('% above threshold', color='y')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    \n",
    "    # plot precision line\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot( pr_thresholds, precision_curve, 'b')\n",
    "    ax2.set_ylabel('precision', color='b')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "\n",
    "    # plot recall line\n",
    "    ax3 = ax2.twinx()\n",
    "    ax3.plot( pr_thresholds, recall_curve, 'r')\n",
    "    ax3.set_ylabel('recall', color='r')\n",
    "    ax3.set_ylim(0,1.05)\n",
    "    \n",
    "    # finish off graph\n",
    "    name = model_name\n",
    "    matplotlib.pyplot.title(name)\n",
    "    \n",
    "    # is there an output path?\n",
    "    if ( ( output_path_IN is not None ) and ( output_path_IN != \"\" ) ):\n",
    "    \n",
    "        # save the figure to file.\n",
    "        matplotlib.pyplot.savefig( output_path_IN )\n",
    "        print( \"In {}: figure output to {}\".format( me, output_path_IN ) )\n",
    "    \n",
    "    #-- END check to see if we output to disk. --#\n",
    "    \n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "    # clear plot.\n",
    "    matplotlib.pyplot.clf()\n",
    "    \n",
    "    return details_OUT\n",
    "    \n",
    "#-- END function plot_precision_recall_n() --#\n",
    "\n",
    "print( \"function plot_precision_recall_n() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function threshold_at_k\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def threshold_at_k( y_scores, k ):\n",
    "    \n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    value_list = None\n",
    "    threshold_index = -1\n",
    "    \n",
    "    # sort values\n",
    "    value_list = numpy.sort( y_scores )\n",
    "    \n",
    "    # reverse order of list\n",
    "    value_list = value_list[ : : -1 ]\n",
    "    \n",
    "    # calculate index of value that is k% of the way through the sorted distribution of scores\n",
    "    threshold_index = int( k * len( y_scores ) )\n",
    "    \n",
    "    # get value that is k% of the way through the sorted distribution of scores\n",
    "    value_OUT = value_list[ threshold_index ]\n",
    "    \n",
    "    print( \"Threshold: {}\".format( value_OUT ) )\n",
    "    \n",
    "    return value_OUT\n",
    "\n",
    "#-- END function threshold_at_k() --#\n",
    "\n",
    "print( \"function threshold_at_k() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function precision_at_k\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def precision_at_k( y_true, y_scores, k ):\n",
    "    \n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    threshold = None\n",
    "    \n",
    "    # get threshold index\n",
    "    threshold = threshold_at_k( y_scores, k )\n",
    "    \n",
    "    # use threshold to generate predicted scores\n",
    "    y_pred = numpy.asarray( [ 1 if i >= threshold else 0 for i in y_scores ] )\n",
    "    \n",
    "    # calculate precision\n",
    "    value_OUT = precision_score( y_true, y_pred )\n",
    "    \n",
    "    return value_OUT\n",
    "\n",
    "#-- END function precision_at_k() --#\n",
    "\n",
    "print( \"function precision_at_k() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function recall_at_k\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def recall_at_k( y_true, y_scores, k ):\n",
    "    \n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    threshold = None\n",
    "    \n",
    "    # get threshold index\n",
    "    threshold = threshold_at_k( y_scores, k )\n",
    "    \n",
    "    # use threshold to generate predicted scores\n",
    "    y_pred = numpy.asarray( [ 1 if i >= threshold else 0 for i in y_scores ] )\n",
    "    \n",
    "    # calculate recall\n",
    "    value_OUT = recall_score( y_true, y_pred )\n",
    "    \n",
    "    return value_OUT\n",
    "\n",
    "#-- END function recall_at_k() --#\n",
    "\n",
    "print( \"function recall_at_k() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function accuracy_at_k\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def accuracy_at_k( y_true, y_scores, k ):\n",
    "    \n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    threshold = None\n",
    "    \n",
    "    # get threshold index\n",
    "    threshold = threshold_at_k( y_scores, k )\n",
    "    \n",
    "    # use threshold to generate predicted scores\n",
    "    y_pred = numpy.asarray( [ 1 if i >= threshold else 0 for i in y_scores ] )\n",
    "    \n",
    "    # calculate accuracy\n",
    "    value_OUT = accuracy_score( y_true, y_pred )\n",
    "    \n",
    "    return value_OUT\n",
    "\n",
    "#-- END function accuracy_at_k() --#\n",
    "\n",
    "print( \"function accuracy_at_k() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function precision_recall_f1\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculation methods\n",
    "CALCULATION_METHOD_DEFAULT = \"default\"\n",
    "CALCULATION_METHOD_BINARY = \"binary\"\n",
    "CACLULATION_METHOD_MACRO = \"macro\"\n",
    "CALCULATION_METHOD_MICRO = \"micro\"\n",
    "CALCULATION_METHOD_WEIGHTED = \"weighted\"\n",
    "\n",
    "# return items\n",
    "RETURN_CONFUSION_MATRIX = \"confusion_matrix\"\n",
    "RETURN_METHOD_TO_RESULT_MAP = \"method_to_result_map\"\n",
    "RETURN_LINE_LIST = \"line_list\"\n",
    "\n",
    "def precision_recall_f1( baseline_list_IN, predicted_list_IN, calculation_methods_list_IN, do_print_IN = True, output_to_file_IN = output_to_file ):\n",
    "\n",
    "    # return reference\n",
    "    output_dict_OUT = {}\n",
    "    \n",
    "    # declare variables\n",
    "    output_string = None\n",
    "    my_line_list = None\n",
    "    calculation_methods = None\n",
    "    cm = None\n",
    "    method_to_result_map = None\n",
    "    calculation_method = None\n",
    "    precision = None\n",
    "    recall = None\n",
    "    accuracy = None\n",
    "    F1 = None\n",
    "    support = None\n",
    "    \n",
    "    # declare variables - default algorithm\n",
    "    default_evaluation = None\n",
    "    default_precision_list = None\n",
    "    default_recall_list = None\n",
    "    default_F1_list = None\n",
    "    default_support_list = None\n",
    "    precision_list_length = None\n",
    "    recall_list_length = None\n",
    "    F1_list_length = None\n",
    "\n",
    "    # init\n",
    "    my_line_list = []\n",
    "    \n",
    "    # init - calculation methods to include and lists\n",
    "    calculation_methods = calculation_methods_list_IN\n",
    "    baseline_list = baseline_list_IN\n",
    "    derived_binary_list = predicted_list_IN\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = metrics.confusion_matrix( baseline_list, derived_binary_list )\n",
    "    \n",
    "    # RETURN - store confusion matrix\n",
    "    output_dict_OUT[ RETURN_CONFUSION_MATRIX ] = cm\n",
    "\n",
    "    # output\n",
    "    output_string = \"\\nConfusion matrix:\\n{}\\n\\nBinary Key:\\n[[ TN, FP ]\\n [ FN, TP ]]\".format( cm )\n",
    "    if ( do_print_IN == True ):\n",
    "        print( output_string )\n",
    "    #-- END if do_print_IN --#\n",
    "\n",
    "    # if output to file...\n",
    "    if ( output_to_file_IN == True ):\n",
    "\n",
    "        # store line for output\n",
    "        my_line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "\n",
    "    # loop over calculation methods\n",
    "    method_to_result_map = {}\n",
    "    for calculation_method in calculation_methods:\n",
    "        \n",
    "        # RETURN - create map for method\n",
    "        \n",
    "\n",
    "        # output\n",
    "        output_string = \"\\n==> {}\".format( calculation_method )\n",
    "        if ( do_print_IN == True ):\n",
    "            print( output_string )\n",
    "        #-- END if do_print_IN --#\n",
    "\n",
    "        # if output to file...\n",
    "        if ( output_to_file_IN == True ):\n",
    "\n",
    "            # store line for output\n",
    "            my_line_list.append( output_string )\n",
    "\n",
    "        #-- END if output... --#\n",
    "\n",
    "        # binary?  If so, do basic calculations as sanity check.\n",
    "        if ( calculation_method == CALCULATION_METHOD_BINARY ):\n",
    "\n",
    "            # calculate precision, recall, accuracy...\n",
    "\n",
    "            # ==> precision\n",
    "            precision = metrics.precision_score( baseline_list, derived_binary_list )\n",
    "\n",
    "            # output\n",
    "            output_string = \"\\n- {} metrics.precision_score = {}\".format( calculation_method, precision )\n",
    "            if ( do_print_IN == True ):\n",
    "                print( output_string )\n",
    "            #-- END if do_print_IN --#\n",
    "\n",
    "            # if output...\n",
    "            if ( output_to_file_IN == True ):\n",
    "\n",
    "                # store line for output\n",
    "                my_line_list.append( output_string )\n",
    "\n",
    "            #-- END if output... --#\n",
    "\n",
    "            # ==> recall\n",
    "            recall = metrics.recall_score( baseline_list, derived_binary_list )\n",
    "\n",
    "            # output\n",
    "            output_string = \"- {} metrics.recall_score = {}\".format( calculation_method, recall )\n",
    "            if ( do_print_IN == True ):\n",
    "                print( output_string )\n",
    "            #-- END if do_print_IN --#\n",
    "\n",
    "            # if output...\n",
    "            if ( output_to_file_IN == True ):\n",
    "\n",
    "                # store line for output\n",
    "                my_line_list.append( output_string )\n",
    "\n",
    "            #-- END if output... --#\n",
    "\n",
    "            # ==> accuracy\n",
    "            accuracy = metrics.accuracy_score( baseline_list, derived_binary_list )\n",
    "\n",
    "            # output\n",
    "            output_string = \"- {} metrics.accuracy_score = {}\".format( calculation_method, accuracy )\n",
    "            if ( do_print_IN == True ):\n",
    "                print( output_string )\n",
    "            #-- END if do_print_IN --#\n",
    "\n",
    "            # if output...\n",
    "            if ( output_to_file_IN == True ):\n",
    "\n",
    "                # store line for output\n",
    "                my_line_list.append( output_string )\n",
    "\n",
    "            #-- END if output... --#\n",
    "\n",
    "        #-- END check to see if CALCULATION_METHOD_BINARY --#\n",
    "\n",
    "        # calculate based on calculation method.\n",
    "\n",
    "        # default?\n",
    "        if ( calculation_method == CALCULATION_METHOD_DEFAULT ):\n",
    "\n",
    "            # default metrics and F-Score - default returns a list for each of\n",
    "            #     the scores per label, so get list and output, don't pick one or\n",
    "            #     another value.\n",
    "            default_evaluation = metrics.precision_recall_fscore_support( baseline_list, derived_binary_list )\n",
    "            default_precision_list = default_evaluation[ 0 ]\n",
    "            default_recall_list = default_evaluation[ 1 ]\n",
    "            default_F1_list = default_evaluation[ 2 ]\n",
    "            default_support_list = default_evaluation[ 3 ]\n",
    "\n",
    "            # output lists\n",
    "            output_string = \"\\ndefault lists:\"\n",
    "            output_string += \"\\n- precision list = {}\".format( default_precision_list )\n",
    "            output_string += \"\\n- recall list = {}\".format( default_recall_list )\n",
    "            output_string += \"\\n- F1 list = {}\".format( default_F1_list )\n",
    "            output_string += \"\\n- support list = {}\".format( default_support_list )\n",
    "\n",
    "            # add to results map\n",
    "            method_to_result_map[ calculation_method ] = default_evaluation\n",
    "\n",
    "            # look at length of lists (should all be the same).\n",
    "            precision_list_length = len( default_precision_list )\n",
    "            recall_list_length = len( default_recall_list )\n",
    "            F1_list_length = len( default_F1_list )\n",
    "\n",
    "            output_string += \"\\n\\nlist lengths: {}\".format( precision_list_length )\n",
    "\n",
    "            if ( precision_list_length > 2 ):\n",
    "\n",
    "                # binary, but list is greater than 2, not binary - output message.\n",
    "                output_string += \"\\n- NOTE: default output lists have more than two entries - your data is not binary.\"\n",
    "\n",
    "            #-- END check to see if list length greater than 2 --#\n",
    "\n",
    "            if ( do_print_IN == True ):\n",
    "                print( output_string )\n",
    "            #-- END if do_print_IN --#\n",
    "\n",
    "            # if output...\n",
    "            if ( output_to_file_IN == True ):\n",
    "\n",
    "                # store line for output\n",
    "                my_line_list.append( output_string )\n",
    "\n",
    "            #-- END if output... --#\n",
    "\n",
    "        # all others are just argument to \"average\" parameter, result in one number per\n",
    "        #     derived score.  For now, implement them the same.\n",
    "        else:\n",
    "\n",
    "            # F-Score\n",
    "            evaluation_tuple = metrics.precision_recall_fscore_support( baseline_list, derived_binary_list, average = calculation_method )\n",
    "            precision = evaluation_tuple[ 0 ]\n",
    "            recall = evaluation_tuple[ 1 ]\n",
    "            F1 = evaluation_tuple[ 2 ]\n",
    "            support = evaluation_tuple[ 3 ]\n",
    "\n",
    "            # add to results map\n",
    "            method_to_result_map[ calculation_method ] = evaluation_tuple\n",
    "\n",
    "            # output\n",
    "            output_string = \"\\n{}: precision = {}, recall = {}, F1 = {}, support = {}\".format( calculation_method, precision, recall, F1, support )\n",
    "            if ( do_print_IN == True ):\n",
    "                print( output_string )\n",
    "            #-- END if do_print_IN --#\n",
    "\n",
    "            # if output to file...\n",
    "            if ( output_to_file_IN == True ):\n",
    "\n",
    "                # store line for output\n",
    "                my_line_list.append( output_string )\n",
    "\n",
    "            #-- END if output... --#\n",
    "\n",
    "        #-- END default F-Score --#\n",
    "\n",
    "    #-- END loop over calculation_methods --#\n",
    "\n",
    "    # RETURN - method-to-result map\n",
    "    output_dict_OUT[ RETURN_METHOD_TO_RESULT_MAP ] = method_to_result_map\n",
    "    \n",
    "    # RETURN - store line_list\n",
    "    output_dict_OUT[ RETURN_LINE_LIST ] = my_line_list\n",
    "\n",
    "    return output_dict_OUT\n",
    "    \n",
    "#-- END function precision_recall_f1() --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class CitationCodingEvaluation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from citation_coding_evaluation import CitationCodingEvaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load JSON files\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "# for in-repo development\n",
    "#baseline_json_path = \"/work/evaluate/data/holdout/data/input/data_set_citations.json\"\n",
    "# set to \"../../data/output\" for running against in-repo code development\n",
    "#derived_prefix = \"../../data/output\"\n",
    "\n",
    "# evaluating models against phase 1 holdout\n",
    "baseline_json_path = \"/work/evaluate/data/holdout/data/input/data_set_citations.json\"\n",
    "derived_prefix = \"/work/evaluate/{}/{}/results/holdout/output\".format( TEAM_NAME, SUBMISSION_FOLDER )\n",
    "\n",
    "derived_json_path = \"{}/data_set_citations.json\".format( derived_prefix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the baseline JSON\n",
    "baseline_json_file = None\n",
    "baseline_json = None\n",
    "\n",
    "# if output...\n",
    "output_string = \"Reading baseline/ground_truth file: {}\".format( baseline_json_path )\n",
    "print( output_string )\n",
    "\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "\n",
    "#-- END if output to file... --#\n",
    "\n",
    "# baseline\n",
    "with open( baseline_json_path ) as baseline_json_file:\n",
    "\n",
    "    # load the JSON from the file.\n",
    "    baseline_json = json.load( baseline_json_file )\n",
    "\n",
    "#-- END with...as --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the derived JSON\n",
    "derived_json_file = None\n",
    "derived_json = None\n",
    "\n",
    "# if output...\n",
    "output_string = \"Reading derived/predicted file: {}\".format( derived_json_path )\n",
    "print( output_string )\n",
    "\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "    \n",
    "#-- END if output to file... --#\n",
    "\n",
    "# baseline\n",
    "with open( derived_json_path ) as derived_json_file:\n",
    "\n",
    "    # load the JSON from the file.\n",
    "    derived_json = json.load( derived_json_file )\n",
    "\n",
    "#-- END with...as --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derived_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all publications\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process JSON\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# init class to handle evaluation\n",
    "coding_evaluator = CitationCodingEvaluation()\n",
    "coding_evaluator.debug_flag = debug_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter publications?\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If we want to only run on a subset of publications, need to use some django code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### django init\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_to_django_init = \"/home/context/django_init.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run $path_to_django_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get IDs of included publications\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sourcenet.models import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with django-taggit**\n",
    "\n",
    "- django-taggit documentation: https://github.com/alex/django-taggit\n",
    "\n",
    "Adding tags to a model:\n",
    "\n",
    "    from django.db import models\n",
    "    \n",
    "    from taggit.managers import TaggableManager\n",
    "    \n",
    "    class Food(models.Model):\n",
    "        # ... fields here\n",
    "    \n",
    "        tags = TaggableManager()\n",
    "    \n",
    "Interacting with a model that has tags:\n",
    "\n",
    "    >>> apple = Food.objects.create(name=\"apple\")\n",
    "    >>> apple.tags.add(\"red\", \"green\", \"delicious\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: green>, <Tag: delicious>]\n",
    "    >>> apple.tags.remove(\"green\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: delicious>]\n",
    "    >>> Food.objects.filter(tags__name__in=[\"red\"])\n",
    "    [<Food: apple>, <Food: cherry>]\n",
    "    \n",
    "    # include only those with certain tags.\n",
    "    #tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "    tags_in_list = [ \"grp_month\", ]\n",
    "    if ( len( tags_in_list ) > 0 ):\n",
    "    \n",
    "        # filter\n",
    "        print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "        grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "    \n",
    "    #-- END check to see if we have a specific list of tags we want to include --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "article_tag_include_list = None\n",
    "article_tag_exclude_list = None\n",
    "article_qs = None\n",
    "article_id_include_list = None\n",
    "article_instance = None\n",
    "\n",
    "# set tags we are filtering on.\n",
    "\n",
    "# include...\n",
    "article_tag_include_list = []\n",
    "article_tag_include_list.append( \"holdout\" )\n",
    "\n",
    "# exclude...\n",
    "article_tag_exclude_list = []\n",
    "article_tag_exclude_list.append( \"no_data\" )\n",
    "\n",
    "# filter Article QuerySet\n",
    "article_qs = Article.objects.filter( tags__name__in = article_tag_include_list )\n",
    "article_qs = article_qs.exclude( tags__name__in = article_tag_exclude_list )\n",
    "\n",
    "# make list of IDs\n",
    "article_id_include_list = []\n",
    "for article_instance in article_qs:\n",
    "    \n",
    "    # get ID\n",
    "    article_id = article_instance.id\n",
    "    \n",
    "    # add to list\n",
    "    article_id_include_list.append( article_id )\n",
    "    \n",
    "#-- END loop over matching articles. --#\n",
    "\n",
    "# store details in coding_evaluator\n",
    "coding_evaluator.set_excluded_article_tag_list( article_tag_exclude_list )\n",
    "coding_evaluator.set_included_article_tag_list( article_tag_include_list )\n",
    "coding_evaluator.set_included_article_id_list( article_id_include_list )\n",
    "\n",
    "print( \"Including {} publications (include tags: {}; exclude tags {}).\".format( len( article_id_include_list ), article_tag_include_list, article_tag_exclude_list ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process JSON files\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# process baseline JSON\n",
    "result_type = CitationCodingEvaluation.RESULT_TYPE_BASELINE\n",
    "citation_json = baseline_json\n",
    "status = coding_evaluator.process_citation_json( citation_json, result_type )\n",
    "\n",
    "# output\n",
    "output_string = \"Processing status for {} (None = Success!): \\\"{}\\\"\".format( result_type, status )\n",
    "print( output_string )\n",
    "\n",
    "# if output...\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "    \n",
    "#-- END if output... --#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# process derived JSON\n",
    "result_type = CitationCodingEvaluation.RESULT_TYPE_DERIVED\n",
    "citation_json = derived_json\n",
    "status = coding_evaluator.process_citation_json( citation_json, result_type )\n",
    "\n",
    "# output\n",
    "output_string = \"Processing status for {} (None = Success!): \\\"{}\\\"\".format( result_type, status )\n",
    "print( output_string )\n",
    "\n",
    "# if output...\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "    \n",
    "#-- END if output... --#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create lists\n",
    "coding_evaluator.debug_flag = False\n",
    "details = coding_evaluator.create_evaluation_lists()\n",
    "status = details\n",
    "baseline_list = coding_evaluator.get_baseline_list()\n",
    "derived_raw_list = coding_evaluator.get_derived_raw_list()\n",
    "derived_binary_list = coding_evaluator.get_derived_binary_list()\n",
    "publication_id_per_citation_list = coding_evaluator.get_publication_id_list()\n",
    "data_set_id_per_citation_list = coding_evaluator.get_data_set_id_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision, recall, and accuracy\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculation methods to include\n",
    "calculation_methods = []\n",
    "calculation_methods.append( CALCULATION_METHOD_DEFAULT )\n",
    "calculation_methods.append( CALCULATION_METHOD_BINARY )\n",
    "#calculation_methods.append( CACLULATION_METHOD_MACRO )\n",
    "#calculation_methods.append( CALCULATION_METHOD_MICRO )\n",
    "#calculation_methods.append( CALCULATION_METHOD_WEIGHTED )\n",
    "\n",
    "# call function to do work.\n",
    "output_dictionary = precision_recall_f1( baseline_list, derived_binary_list, calculation_methods )\n",
    "\n",
    "# add lines from output to line_list\n",
    "line_list = line_list + output_dictionary.get( \"line_list\", [] )\n",
    "line_list.append( \"\\n\" )\n",
    "\n",
    "print( \"----> output dictionary: {}\".format( output_dictionary ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check for excluded articles\n",
    "excluded_article_count = details.get( CitationCodingEvaluation.DETAILS_EXCLUDED_ARTICLE_COUNT, None )\n",
    "excluded_article_id_list = details.get( CitationCodingEvaluation.DETAILS_EXCLUDED_ARTICLE_ID_LIST, None )\n",
    "if ( ( excluded_article_count is not None ) and ( excluded_article_count > 0 ) ):\n",
    "    \n",
    "    # add excluded articles details:\n",
    "    line_list.append( \"\\n\" )\n",
    "    line_string = \"{} excluded publications: {} \".format( excluded_article_count, excluded_article_id_list )\n",
    "    line_list.append( line_string )\n",
    "    print( line_string )\n",
    "    \n",
    "#-- END check for excluded publications. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision, recall, and accuracy per publication\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "pub_debug_flag = False\n",
    "publication_to_lists_map = coding_evaluator.get_lists_by_publication()\n",
    "pub_publication_id_list = []\n",
    "pub_false_positive_list = []\n",
    "pub_false_negative_list = []\n",
    "pub_calculation_methods = []\n",
    "#calculation_methods.append( CALCULATION_METHOD_DEFAULT )\n",
    "pub_calculation_methods.append( CALCULATION_METHOD_BINARY )\n",
    "#calculation_methods.append( CACLULATION_METHOD_MACRO )\n",
    "#calculation_methods.append( CALCULATION_METHOD_MICRO )\n",
    "#calculation_methods.append( CALCULATION_METHOD_WEIGHTED )\n",
    "pub_output_dictionary = None\n",
    "pub_confusion_matrix = None\n",
    "pub_false_positive_count = None\n",
    "pub_false_negative_count = None\n",
    "output_string = None\n",
    "\n",
    "# declare variables - precision/recall/F1\n",
    "pub_method_to_result_map = None\n",
    "pub_evaluation_tuple = None\n",
    "pub_precision = None\n",
    "pub_recall = None\n",
    "pub_F1 = None\n",
    "pub_precision_list = []\n",
    "pub_recall_list = []\n",
    "pub_F1_list = []\n",
    "\n",
    "# loop over publications\n",
    "for publication_id in six.iterkeys( publication_to_lists_map ):\n",
    "    \n",
    "    if ( debug_flag == True ):\n",
    "        print( \"Publication ID: {}\".format( publication_id ) )\n",
    "    #-- END debug --#\n",
    "    \n",
    "    # get lists\n",
    "    pub_list_dictionary = publication_to_lists_map.get( publication_id, None )\n",
    "    pub_baseline_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_BASELINE, None )\n",
    "    pub_derived_binary_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_DERIVED_BINARY, None )\n",
    "    pub_derived_raw_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_DERIVED_RAW, None )\n",
    "    pub_publication_id_per_citation_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_PUBLICATION_ID, None )\n",
    "    pub_data_set_id_per_citation_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_DATA_SET_ID, None )\n",
    "    \n",
    "    if ( debug_flag == True ):\n",
    "        # print lists:\n",
    "        print( \"====> baseline......: {}\".format( pub_baseline_list ) )\n",
    "        print( \"====> derived_binary: {}\".format( pub_derived_binary_list ) )\n",
    "        print( \"====> derived_raw...: {}\".format( pub_derived_raw_list ) )\n",
    "        print( \"====> publication_id: {}\".format( pub_publication_id_per_citation_list ) )\n",
    "        print( \"====> data_set_id...: {}\".format( pub_data_set_id_per_citation_list ) )\n",
    "    #-- END debug --#\n",
    "    \n",
    "    # call the precision and recall function\n",
    "    pub_output_dictionary = precision_recall_f1( pub_baseline_list, pub_derived_binary_list, pub_calculation_methods, do_print_IN = pub_debug_flag )\n",
    "\n",
    "    if ( debug_flag == True ):\n",
    "        print( \"----> pub output dictionary: {}\".format( pub_output_dictionary ) )\n",
    "    #-- END debug --#\n",
    "    \n",
    "    # get confusion matrix\n",
    "    pub_confusion_matrix = pub_output_dictionary.get( RETURN_CONFUSION_MATRIX, None )\n",
    "    if ( debug_flag == True ):\n",
    "        print( \"Confusion Matrix: {}\".format( pub_confusion_matrix ) )\n",
    "    #-- END debug --#\n",
    "            \n",
    "    \n",
    "    if ( pub_confusion_matrix is not None ):\n",
    "        \n",
    "        # try to get false positives (cm[ 0 ][ 1 ]).  If exception, is 0.\n",
    "        try:\n",
    "            \n",
    "            pub_false_positive_count = pub_confusion_matrix[ 0 ][ 1 ]\n",
    "\n",
    "            if ( debug_flag == True ):\n",
    "                print( \"found FP!\" )\n",
    "            #-- END debug --#\n",
    "            \n",
    "        except:\n",
    "\n",
    "            if ( debug_flag == True ):\n",
    "                print( \"no FP!\" )\n",
    "            #-- END debug --#\n",
    "            \n",
    "            # index doesn't exist.  Set to 0.\n",
    "            pub_false_positive_count = 0\n",
    "            \n",
    "        #-- END try...except. --#\n",
    "        \n",
    "        # try to get false negatives (cm[ 1 ][ 0 ]).  If exception, is 0.\n",
    "        try:\n",
    "            \n",
    "            pub_false_negative_count = pub_confusion_matrix[ 1 ][ 0 ]\n",
    "\n",
    "            if ( debug_flag == True ):\n",
    "                print( \"found FN!\" )\n",
    "            #-- END debug --#\n",
    "            \n",
    "        except:\n",
    "\n",
    "            if ( debug_flag == True ):\n",
    "                print( \"no FN!\" )\n",
    "            #-- END debug --#\n",
    "            \n",
    "            # index doesn't exist.  Set to 0.\n",
    "            pub_false_negative_count = 0\n",
    "            \n",
    "        #-- END try...except. --#\n",
    "\n",
    "        # add id and count to list.\n",
    "        pub_publication_id_list.append( publication_id )\n",
    "        pub_false_positive_list.append( pub_false_positive_count )\n",
    "        pub_false_negative_list.append( pub_false_negative_count )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # no confusion matrix\n",
    "        print( \"ERROR - no confusion matrix!\" )\n",
    "        \n",
    "    #-- END check to see if confusion matrix --#\n",
    "    \n",
    "    # get results...\n",
    "    pub_precision = -1\n",
    "    pub_recall = -1\n",
    "    pub_F1 = -1\n",
    "    pub_method_to_result_map = pub_output_dictionary.get( RETURN_METHOD_TO_RESULT_MAP, None )\n",
    "    if ( pub_method_to_result_map is not None ):\n",
    "        \n",
    "        # get results... for binary calculation method\n",
    "        pub_evaluation_tuple = pub_method_to_result_map.get( CALCULATION_METHOD_BINARY, None )\n",
    "        if ( pub_evaluation_tuple is not None ):\n",
    "            \n",
    "            # get results\n",
    "            pub_precision = pub_evaluation_tuple[ 0 ]\n",
    "            pub_recall = pub_evaluation_tuple[ 1 ]\n",
    "            pub_F1 = pub_evaluation_tuple[ 2 ]\n",
    "    \n",
    "        else:\n",
    "\n",
    "            # no results for binary calculation method\n",
    "            print( \"ERROR - no results for binary calculation method!\" )\n",
    "\n",
    "        #-- END check to see if confusion matrix --#\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # no results for binary calculation method\n",
    "        print( \"ERROR - no results!\" )\n",
    "        \n",
    "    #-- END check to see if confusion matrix --#\n",
    "    \n",
    "    # add to lists\n",
    "    pub_precision_list.append( pub_precision )\n",
    "    pub_recall_list.append( pub_recall )\n",
    "    pub_F1_list.append( pub_F1 )\n",
    "    \n",
    "#-- END loop over per-publication lists --#\n",
    "\n",
    "if ( debug_flag == True ):\n",
    "    print( pub_publication_id_list )\n",
    "    print( pub_false_positive_list )\n",
    "    print( pub_false_negative_list )\n",
    "#-- END debug --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### false positives (FP)\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize\n",
    "output_string = \"\\n========================================\\nFalse Positives (FP):\"\n",
    "print( output_string )\n",
    "line_list.append( output_string )\n",
    "\n",
    "# declare variables\n",
    "pub_false_positive_array = None\n",
    "pub_false_positive_mean = None\n",
    "pub_fp_pub_id_list = []\n",
    "pub_fp_count_list = []\n",
    "item_index = None\n",
    "current_count = None\n",
    "output_string = None\n",
    "zipped_fp_lists = None\n",
    "fp_row = None\n",
    "\n",
    "# convert false positive list to a numpy array and get the mean\n",
    "pub_false_positive_array = numpy.array( pub_false_positive_list )\n",
    "pub_false_positive_mean = numpy.mean( pub_false_positive_array )\n",
    "\n",
    "# loop over items, flag any that are over mean\n",
    "item_index = -1\n",
    "for current_count in pub_false_positive_list:\n",
    "    \n",
    "    # increment index\n",
    "    item_index += 1\n",
    "\n",
    "    # get publication ID\n",
    "    publication_id = pub_publication_id_list[ item_index ]\n",
    "    \n",
    "    # is count greater than mean?\n",
    "    if ( current_count > pub_false_positive_mean ):\n",
    "        \n",
    "        # add to list\n",
    "        pub_fp_pub_id_list.append( publication_id )\n",
    "        pub_fp_count_list.append( current_count )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if ( debug_flag == True ):\n",
    "            print( \"- pub {} FP {} <= mean ( {} )\".format( publication_id, current_count, pub_false_positive_mean ) )\n",
    "        #-- END debug --#\n",
    "        \n",
    "    #-- END loop over false positives. --#\n",
    "    \n",
    "#-- END loop over publications --#\n",
    "\n",
    "# zip up the two lists (one list of pairs of values, rather than two lists).\n",
    "\n",
    "# ID order\n",
    "zipped_fp_lists = list( zip( pub_fp_count_list, pub_fp_pub_id_list ) )\n",
    "\n",
    "# convert to ordered by count, then ID, largest to smallest.\n",
    "zipped_fp_lists.sort( reverse = True )\n",
    "\n",
    "# anything in the list?\n",
    "fp_count = len( zipped_fp_lists )\n",
    "if( fp_count > 0 ):\n",
    "\n",
    "    output_string = \"\\n==> {} False Positives (FP) above mean ( {} ):\".format( fp_count, pub_false_positive_mean )\n",
    "    print( output_string )\n",
    "    line_list.append( output_string )\n",
    "\n",
    "    # output for review\n",
    "    for fp_row in zipped_fp_lists:\n",
    "\n",
    "        # summarize\n",
    "        output_string = \"- pub {} FP {} > mean ( {} )\".format( fp_row[ 1 ], fp_row[ 0 ], pub_false_positive_mean )\n",
    "        print( output_string )\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END loop over items --#\n",
    "\n",
    "#-- END check to see if anything in list. --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### false negatives (FN)\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize\n",
    "output_string = \"\\n========================================\\nFalse Negatives (FN):\"\n",
    "print( output_string )\n",
    "line_list.append( output_string )\n",
    "\n",
    "# declare variables\n",
    "pub_false_negative_array = None\n",
    "pub_false_negative_mean = None\n",
    "pub_fn_pub_id_list = []\n",
    "pub_fn_count_list = []\n",
    "item_index = None\n",
    "current_count = None\n",
    "output_string = None\n",
    "zipped_fn_lists = None\n",
    "fn_row = None\n",
    "\n",
    "# convert false negative list to a numpy array and get the mean\n",
    "pub_false_negative_array = numpy.array( pub_false_negative_list )\n",
    "pub_false_negative_mean = numpy.mean( pub_false_negative_array )\n",
    "\n",
    "# loop over items, flag any that are over mean\n",
    "item_index = -1\n",
    "for current_count in pub_false_negative_list:\n",
    "    \n",
    "    # increment index\n",
    "    item_index += 1\n",
    "\n",
    "    # get publication ID\n",
    "    publication_id = pub_publication_id_list[ item_index ]\n",
    "    \n",
    "    # is count greater than mean?\n",
    "    if ( current_count > pub_false_negative_mean ):\n",
    "        \n",
    "        # add to list\n",
    "        pub_fn_pub_id_list.append( publication_id )\n",
    "        pub_fn_count_list.append( current_count )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if ( debug_flag == True ):\n",
    "            print( \"- pub {} FN {} <= mean ( {} )\".format( publication_id, current_count, pub_false_negative_mean ) )\n",
    "        #-- END debug --#\n",
    "        \n",
    "    #-- END loop over false negatives. --#\n",
    "    \n",
    "#-- END loop over publications --#\n",
    "\n",
    "# zip up the two lists (one list of pairs of values, rather than two lists).\n",
    "\n",
    "# ID order\n",
    "zipped_fn_lists = list( zip( pub_fn_count_list, pub_fn_pub_id_list ) )\n",
    "\n",
    "# convert to ordered by count, then ID, largest to smallest.\n",
    "zipped_fn_lists.sort( reverse = True )\n",
    "\n",
    "# anything in the list?\n",
    "fn_count = len( zipped_fn_lists )\n",
    "if( fn_count > 0 ):\n",
    "\n",
    "    output_string = \"\\n==> {} False Negatives (FN) above mean ( {} ):\".format( fn_count, pub_false_negative_mean )\n",
    "    print( output_string )\n",
    "    line_list.append( output_string )\n",
    "\n",
    "    # output for review\n",
    "    for fn_row in zipped_fn_lists:\n",
    "\n",
    "        # summarize\n",
    "        output_string = \"- pub {} FN {} > mean ( {} )\".format( fn_row[ 1 ], fn_row[ 0 ], pub_false_negative_mean )\n",
    "        print( output_string )\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END loop over items --#\n",
    "    \n",
    "#-- END check to see if anything in list. --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output all publication-citation pairs\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# output all the full lists\n",
    "current_index = -1\n",
    "for item in publication_id_per_citation_list:\n",
    "    \n",
    "    # increment index\n",
    "    current_index += 1\n",
    "    \n",
    "    # get current values.\n",
    "    baseline_value = baseline_list[ current_index ]\n",
    "    derived_raw_value = derived_raw_list[ current_index ]\n",
    "    derived_binary_value = derived_binary_list[ current_index ]\n",
    "    pub_id_value = publication_id_per_citation_list[ current_index ]\n",
    "    data_set_id_value = data_set_id_per_citation_list[ current_index ]\n",
    "    \n",
    "    print( \"{}: pub ID {} - data set ID {} - baseline {} - binary {} - raw {}\".format( current_index, pub_id_value, data_set_id_value, baseline_value, derived_binary_value, derived_raw_value ) )\n",
    "    \n",
    "#-- END loop over full lists. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph precision and recall at n\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set precision_recall_graph_path\n",
    "#precision_recall_graph_path = \"{}/{}precision_recall_graph{}.pdf\".format( output_folder_path, file_name_prefix, file_name_suffix )\n",
    "\n",
    "# declare variables\n",
    "plot_details = None\n",
    "\n",
    "# output to file?\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # output figure to file\n",
    "    plot_details = plot_precision_recall_n( baseline_list, derived_raw_list, \"evaluation\", output_path_IN = precision_recall_graph_path )\n",
    "\n",
    "else:\n",
    "    \n",
    "    # just output to standard out (as is possible)\n",
    "    plot_details = plot_precision_recall_n( baseline_list, derived_raw_list, \"evaluation\" )\n",
    "\n",
    "#-- END check to see if output graph to file --#\n",
    "\n",
    "# DEBUG?\n",
    "if ( debug_flag == True ):\n",
    "\n",
    "    # summarize\n",
    "    output_string = \"- plot details: {}\".format( plot_details )\n",
    "    print( output_string )\n",
    "    #line_list.append( output_string )\n",
    "\n",
    "#-- END DEBUG --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output results to file\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set results file path:\n",
    "#results_file_path = \"{}/{}evaluation_results{}.txt\".format( output_folder_path, file_name_prefix, file_name_suffix )\n",
    "\n",
    "# declare variables\n",
    "results_file = None\n",
    "line_list_string = None\n",
    "\n",
    "# do we output to file?\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    if ( debug_flag == True ):\n",
    "        print( line_list )\n",
    "    #-- END check to see if debug --#\n",
    "    \n",
    "    # yes.  open output file.\n",
    "    with open( results_file_path, mode = \"w\" ) as results_file:\n",
    "\n",
    "        # join line list with \"/n\", then write.\n",
    "        line_list_string = \"\\n\".join( line_list )\n",
    "        results_file.write( line_list_string )\n",
    "\n",
    "    #-- END with...as --#\n",
    "    \n",
    "    print( \"results output to {}\".format( results_file_path ) )\n",
    "    \n",
    "#-- END check to see if we output to file --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate only publications with citations\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now, try just doing this on publications that have citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset line_list.\n",
    "line_list = []\n",
    "\n",
    "# if output...\n",
    "output_string = \"Using baseline/ground_truth file: {}\".format( baseline_json_path )\n",
    "print( output_string )\n",
    "\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "\n",
    "#-- END if output to file... --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process JSON\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# init class to handle evaluation\n",
    "coding_evaluator = CitationCodingEvaluation()\n",
    "coding_evaluator.debug_flag = debug_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter publications?\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "If we want to only run on a subset of publications, need to use some django code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### django init\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_to_django_init = \"/home/context/django_init.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run $path_to_django_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get IDs of included publications\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sourcenet.models import Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working with django-taggit**\n",
    "\n",
    "- django-taggit documentation: https://github.com/alex/django-taggit\n",
    "\n",
    "Adding tags to a model:\n",
    "\n",
    "    from django.db import models\n",
    "    \n",
    "    from taggit.managers import TaggableManager\n",
    "    \n",
    "    class Food(models.Model):\n",
    "        # ... fields here\n",
    "    \n",
    "        tags = TaggableManager()\n",
    "    \n",
    "Interacting with a model that has tags:\n",
    "\n",
    "    >>> apple = Food.objects.create(name=\"apple\")\n",
    "    >>> apple.tags.add(\"red\", \"green\", \"delicious\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: green>, <Tag: delicious>]\n",
    "    >>> apple.tags.remove(\"green\")\n",
    "    >>> apple.tags.all()\n",
    "    [<Tag: red>, <Tag: delicious>]\n",
    "    >>> Food.objects.filter(tags__name__in=[\"red\"])\n",
    "    [<Food: apple>, <Food: cherry>]\n",
    "    \n",
    "    # include only those with certain tags.\n",
    "    #tags_in_list = [ \"prelim_unit_test_001\", \"prelim_unit_test_002\", \"prelim_unit_test_003\", \"prelim_unit_test_004\", \"prelim_unit_test_005\", \"prelim_unit_test_006\", \"prelim_unit_test_007\" ]\n",
    "    tags_in_list = [ \"grp_month\", ]\n",
    "    if ( len( tags_in_list ) > 0 ):\n",
    "    \n",
    "        # filter\n",
    "        print( \"filtering to just articles with tags: \" + str( tags_in_list ) )\n",
    "        grp_article_qs = grp_article_qs.filter( tags__name__in = tags_in_list )\n",
    "    \n",
    "    #-- END check to see if we have a specific list of tags we want to include --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to just those in holdout that have one or more related citations.\n",
    "\n",
    "# declare variables\n",
    "article_tag_include_list = None\n",
    "article_tag_exclude_list = None\n",
    "article_qs = None\n",
    "\n",
    "# include...\n",
    "article_tag_include_list = []\n",
    "article_tag_include_list.append( \"holdout\" )\n",
    "\n",
    "# exclude...\n",
    "article_tag_exclude_list = []\n",
    "article_tag_exclude_list.append( \"wc_holdout\" )\n",
    "\n",
    "# get all articles\n",
    "article_qs = Article.objects.all()\n",
    "\n",
    "# include tags?\n",
    "if ( ( article_tag_include_list is not None ) and ( len( article_tag_include_list ) > 0 ) ):\n",
    "    \n",
    "    # yes.\n",
    "    article_qs = article_qs.filter( tags__name__in = article_tag_include_list )\n",
    "    \n",
    "#-- END check to see if include tags? --#\n",
    "\n",
    "# exclude tags?\n",
    "if ( ( article_tag_exclude_list is not None ) and ( len( article_tag_exclude_list ) > 0 ) ):\n",
    "    \n",
    "    # yes.\n",
    "    article_qs = article_qs.exclude( tags__name__in = article_tag_exclude_list )\n",
    "    \n",
    "#-- END check to see if include tags? --#\n",
    "\n",
    "# filter down to just those where related data set citation id is > 0.\n",
    "\n",
    "print( \"Filtered to {} Article instances.\".format( article_qs.count() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "article_instance = None\n",
    "citation_count = None\n",
    "article_with_citations_id_list = None\n",
    "\n",
    "# now, loop over the publications\n",
    "article_with_citations_id_list = []\n",
    "for article_instance in article_qs:\n",
    "    \n",
    "    # see how many citations.\n",
    "    citation_count = 0\n",
    "    citation_count = article_instance.datasetcitation_set.all().count()\n",
    "    \n",
    "    if ( citation_count > 0 ):\n",
    "        \n",
    "        # add to id list.\n",
    "        article_with_citations_id_list.append( article_instance.id )\n",
    "        \n",
    "    #-- END check to see if has citations --#\n",
    "    \n",
    "#-- END loop over articles. --#\n",
    "\n",
    "# re-do article_qs, limiting to IDs in our list.\n",
    "article_qs = Article.objects.filter( id__in = article_with_citations_id_list )\n",
    "\n",
    "print( \"Filtered to {} Article instances.\".format( article_qs.count() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "article_id_include_list = None\n",
    "article_instance = None\n",
    "\n",
    "# make list of IDs\n",
    "article_id_include_list = []\n",
    "for article_instance in article_qs:\n",
    "    \n",
    "    # get ID\n",
    "    article_id = article_instance.id\n",
    "    \n",
    "    # add to list\n",
    "    article_id_include_list.append( article_id )\n",
    "    \n",
    "#-- END loop over matching articles. --#\n",
    "\n",
    "# store details in coding_evaluator\n",
    "coding_evaluator.set_excluded_article_tag_list( article_tag_exclude_list )\n",
    "coding_evaluator.set_included_article_tag_list( article_tag_include_list )\n",
    "coding_evaluator.set_included_article_id_list( article_id_include_list )\n",
    "\n",
    "print( \"Including {} publications (include tags: {}; exclude tags {}).\".format( len( article_id_include_list ), article_tag_include_list, article_tag_exclude_list ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process JSON files\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# process baseline JSON\n",
    "result_type = CitationCodingEvaluation.RESULT_TYPE_BASELINE\n",
    "citation_json = baseline_json\n",
    "status = coding_evaluator.process_citation_json( citation_json, result_type )\n",
    "\n",
    "# output\n",
    "output_string = \"Processing status for {} (None = Success!): \\\"{}\\\"\".format( result_type, status )\n",
    "print( output_string )\n",
    "\n",
    "# if output...\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "    \n",
    "#-- END if output... --#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# process derived JSON\n",
    "result_type = CitationCodingEvaluation.RESULT_TYPE_DERIVED\n",
    "citation_json = derived_json\n",
    "status = coding_evaluator.process_citation_json( citation_json, result_type )\n",
    "\n",
    "# output\n",
    "output_string = \"Processing status for {} (None = Success!): \\\"{}\\\"\".format( result_type, status )\n",
    "print( output_string )\n",
    "\n",
    "# if output...\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "    \n",
    "#-- END if output... --#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create lists\n",
    "coding_evaluator.debug_flag = False\n",
    "details = coding_evaluator.create_evaluation_lists()\n",
    "status = details\n",
    "baseline_list = coding_evaluator.get_baseline_list()\n",
    "derived_raw_list = coding_evaluator.get_derived_raw_list()\n",
    "derived_binary_list = coding_evaluator.get_derived_binary_list()\n",
    "publication_id_per_citation_list = coding_evaluator.get_publication_id_list()\n",
    "data_set_id_per_citation_list = coding_evaluator.get_data_set_id_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision, recall, and accuracy\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculation methods to include\n",
    "calculation_methods = []\n",
    "calculation_methods.append( CALCULATION_METHOD_DEFAULT )\n",
    "calculation_methods.append( CALCULATION_METHOD_BINARY )\n",
    "#calculation_methods.append( CACLULATION_METHOD_MACRO )\n",
    "#calculation_methods.append( CALCULATION_METHOD_MICRO )\n",
    "#calculation_methods.append( CALCULATION_METHOD_WEIGHTED )\n",
    "\n",
    "# call function to do work.\n",
    "output_dictionary = precision_recall_f1( baseline_list, derived_binary_list, calculation_methods )\n",
    "\n",
    "# add lines from output to line_list\n",
    "line_list = line_list + output_dictionary.get( \"line_list\", [] )\n",
    "line_list.append( \"\\n\" )\n",
    "\n",
    "print( \"----> output dictionary: {}\".format( output_dictionary ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check for excluded articles\n",
    "excluded_article_count = details.get( CitationCodingEvaluation.DETAILS_EXCLUDED_ARTICLE_COUNT, None )\n",
    "excluded_article_id_list = details.get( CitationCodingEvaluation.DETAILS_EXCLUDED_ARTICLE_ID_LIST, None )\n",
    "if ( ( excluded_article_count is not None ) and ( excluded_article_count > 0 ) ):\n",
    "    \n",
    "    # add excluded articles details:\n",
    "    line_list.append( \"\\n\" )\n",
    "    line_string = \"{} excluded publications: {} \".format( excluded_article_count, excluded_article_id_list )\n",
    "    line_list.append( line_string )\n",
    "    print( line_string )\n",
    "    \n",
    "#-- END check for excluded publications. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision, recall, and accuracy per publication\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# declare variables\n",
    "pub_debug_flag = False\n",
    "publication_to_lists_map = coding_evaluator.get_lists_by_publication()\n",
    "pub_publication_id_list = []\n",
    "pub_false_positive_list = []\n",
    "pub_false_negative_list = []\n",
    "pub_calculation_methods = []\n",
    "#calculation_methods.append( CALCULATION_METHOD_DEFAULT )\n",
    "pub_calculation_methods.append( CALCULATION_METHOD_BINARY )\n",
    "#calculation_methods.append( CACLULATION_METHOD_MACRO )\n",
    "#calculation_methods.append( CALCULATION_METHOD_MICRO )\n",
    "#calculation_methods.append( CALCULATION_METHOD_WEIGHTED )\n",
    "pub_output_dictionary = None\n",
    "pub_confusion_matrix = None\n",
    "pub_false_positive_count = None\n",
    "pub_false_negative_count = None\n",
    "output_string = None\n",
    "\n",
    "# declare variables - precision/recall/F1\n",
    "pub_method_to_result_map = None\n",
    "pub_evaluation_tuple = None\n",
    "pub_precision = None\n",
    "pub_recall = None\n",
    "pub_F1 = None\n",
    "pub_precision_list = []\n",
    "pub_recall_list = []\n",
    "pub_F1_list = []\n",
    "\n",
    "# loop over publications\n",
    "for publication_id in six.iterkeys( publication_to_lists_map ):\n",
    "    \n",
    "    if ( debug_flag == True ):\n",
    "        print( \"Publication ID: {}\".format( publication_id ) )\n",
    "    #-- END debug --#\n",
    "    \n",
    "    # get lists\n",
    "    pub_list_dictionary = publication_to_lists_map.get( publication_id, None )\n",
    "    pub_baseline_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_BASELINE, None )\n",
    "    pub_derived_binary_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_DERIVED_BINARY, None )\n",
    "    pub_derived_raw_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_DERIVED_RAW, None )\n",
    "    pub_publication_id_per_citation_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_PUBLICATION_ID, None )\n",
    "    pub_data_set_id_per_citation_list = pub_list_dictionary.get( coding_evaluator.LIST_TYPE_DATA_SET_ID, None )\n",
    "    \n",
    "    if ( debug_flag == True ):\n",
    "        # print lists:\n",
    "        print( \"====> baseline......: {}\".format( pub_baseline_list ) )\n",
    "        print( \"====> derived_binary: {}\".format( pub_derived_binary_list ) )\n",
    "        print( \"====> derived_raw...: {}\".format( pub_derived_raw_list ) )\n",
    "        print( \"====> publication_id: {}\".format( pub_publication_id_per_citation_list ) )\n",
    "        print( \"====> data_set_id...: {}\".format( pub_data_set_id_per_citation_list ) )\n",
    "    #-- END debug --#\n",
    "    \n",
    "    # call the precision and recall function\n",
    "    pub_output_dictionary = precision_recall_f1( pub_baseline_list, pub_derived_binary_list, pub_calculation_methods, do_print_IN = pub_debug_flag )\n",
    "\n",
    "    if ( debug_flag == True ):\n",
    "        print( \"----> pub output dictionary: {}\".format( pub_output_dictionary ) )\n",
    "    #-- END debug --#\n",
    "    \n",
    "    # get confusion matrix\n",
    "    pub_confusion_matrix = pub_output_dictionary.get( RETURN_CONFUSION_MATRIX, None )\n",
    "    if ( debug_flag == True ):\n",
    "        print( \"Confusion Matrix: {}\".format( pub_confusion_matrix ) )\n",
    "    #-- END debug --#\n",
    "            \n",
    "    \n",
    "    if ( pub_confusion_matrix is not None ):\n",
    "        \n",
    "        # try to get false positives (cm[ 0 ][ 1 ]).  If exception, is 0.\n",
    "        try:\n",
    "            \n",
    "            pub_false_positive_count = pub_confusion_matrix[ 0 ][ 1 ]\n",
    "\n",
    "            if ( debug_flag == True ):\n",
    "                print( \"found FP!\" )\n",
    "            #-- END debug --#\n",
    "            \n",
    "        except:\n",
    "\n",
    "            if ( debug_flag == True ):\n",
    "                print( \"no FP!\" )\n",
    "            #-- END debug --#\n",
    "            \n",
    "            # index doesn't exist.  Set to 0.\n",
    "            pub_false_positive_count = 0\n",
    "            \n",
    "        #-- END try...except. --#\n",
    "        \n",
    "        # try to get false negatives (cm[ 1 ][ 0 ]).  If exception, is 0.\n",
    "        try:\n",
    "            \n",
    "            pub_false_negative_count = pub_confusion_matrix[ 1 ][ 0 ]\n",
    "\n",
    "            if ( debug_flag == True ):\n",
    "                print( \"found FN!\" )\n",
    "            #-- END debug --#\n",
    "            \n",
    "        except:\n",
    "\n",
    "            if ( debug_flag == True ):\n",
    "                print( \"no FN!\" )\n",
    "            #-- END debug --#\n",
    "            \n",
    "            # index doesn't exist.  Set to 0.\n",
    "            pub_false_negative_count = 0\n",
    "            \n",
    "        #-- END try...except. --#\n",
    "\n",
    "        # add id and count to list.\n",
    "        pub_publication_id_list.append( publication_id )\n",
    "        pub_false_positive_list.append( pub_false_positive_count )\n",
    "        pub_false_negative_list.append( pub_false_negative_count )\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # no confusion matrix\n",
    "        print( \"ERROR - no confusion matrix!\" )\n",
    "        \n",
    "    #-- END check to see if confusion matrix --#\n",
    "    \n",
    "    # get results...\n",
    "    pub_precision = -1\n",
    "    pub_recall = -1\n",
    "    pub_F1 = -1\n",
    "    pub_method_to_result_map = pub_output_dictionary.get( RETURN_METHOD_TO_RESULT_MAP, None )\n",
    "    if ( pub_method_to_result_map is not None ):\n",
    "        \n",
    "        # get results... for binary calculation method\n",
    "        pub_evaluation_tuple = pub_method_to_result_map.get( CALCULATION_METHOD_BINARY, None )\n",
    "        if ( pub_evaluation_tuple is not None ):\n",
    "            \n",
    "            # get results\n",
    "            pub_precision = pub_evaluation_tuple[ 0 ]\n",
    "            pub_recall = pub_evaluation_tuple[ 1 ]\n",
    "            pub_F1 = pub_evaluation_tuple[ 2 ]\n",
    "    \n",
    "        else:\n",
    "\n",
    "            # no results for binary calculation method\n",
    "            print( \"ERROR - no results for binary calculation method!\" )\n",
    "\n",
    "        #-- END check to see if confusion matrix --#\n",
    "\n",
    "    else:\n",
    "        \n",
    "        # no results for binary calculation method\n",
    "        print( \"ERROR - no results!\" )\n",
    "        \n",
    "    #-- END check to see if confusion matrix --#\n",
    "    \n",
    "    # add to lists\n",
    "    pub_precision_list.append( pub_precision )\n",
    "    pub_recall_list.append( pub_recall )\n",
    "    pub_F1_list.append( pub_F1 )\n",
    "    \n",
    "#-- END loop over per-publication lists --#\n",
    "\n",
    "if ( debug_flag == True ):\n",
    "    print( pub_publication_id_list )\n",
    "    print( pub_false_positive_list )\n",
    "    print( pub_false_negative_list )\n",
    "#-- END debug --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### false positives (FP)\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize\n",
    "output_string = \"\\n========================================\\nFalse Positives (FP):\"\n",
    "print( output_string )\n",
    "line_list.append( output_string )\n",
    "\n",
    "# declare variables\n",
    "pub_false_positive_array = None\n",
    "pub_false_positive_mean = None\n",
    "pub_fp_pub_id_list = []\n",
    "pub_fp_count_list = []\n",
    "item_index = None\n",
    "current_count = None\n",
    "output_string = None\n",
    "zipped_fp_lists = None\n",
    "fp_row = None\n",
    "\n",
    "# convert false positive list to a numpy array and get the mean\n",
    "pub_false_positive_array = numpy.array( pub_false_positive_list )\n",
    "pub_false_positive_mean = numpy.mean( pub_false_positive_array )\n",
    "\n",
    "# loop over items, flag any that are over mean\n",
    "item_index = -1\n",
    "for current_count in pub_false_positive_list:\n",
    "    \n",
    "    # increment index\n",
    "    item_index += 1\n",
    "\n",
    "    # get publication ID\n",
    "    publication_id = pub_publication_id_list[ item_index ]\n",
    "    \n",
    "    # is count greater than mean?\n",
    "    if ( current_count > pub_false_positive_mean ):\n",
    "        \n",
    "        # add to list\n",
    "        pub_fp_pub_id_list.append( publication_id )\n",
    "        pub_fp_count_list.append( current_count )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if ( debug_flag == True ):\n",
    "            print( \"- pub {} FP {} <= mean ( {} )\".format( publication_id, current_count, pub_false_positive_mean ) )\n",
    "        #-- END debug --#\n",
    "        \n",
    "    #-- END loop over false positives. --#\n",
    "    \n",
    "#-- END loop over publications --#\n",
    "\n",
    "# zip up the two lists (one list of pairs of values, rather than two lists).\n",
    "\n",
    "# ID order\n",
    "zipped_fp_lists = list( zip( pub_fp_count_list, pub_fp_pub_id_list ) )\n",
    "\n",
    "# convert to ordered by count, then ID, largest to smallest.\n",
    "zipped_fp_lists.sort( reverse = True )\n",
    "\n",
    "# anything in the list?\n",
    "fp_count = len( zipped_fp_lists )\n",
    "if( fp_count > 0 ):\n",
    "\n",
    "    output_string = \"\\n==> {} False Positives (FP) above mean ( {} ):\".format( fp_count, pub_false_positive_mean )\n",
    "    print( output_string )\n",
    "    line_list.append( output_string )\n",
    "\n",
    "    # output for review\n",
    "    for fp_row in zipped_fp_lists:\n",
    "\n",
    "        # summarize\n",
    "        output_string = \"- pub {} FP {} > mean ( {} )\".format( fp_row[ 1 ], fp_row[ 0 ], pub_false_positive_mean )\n",
    "        print( output_string )\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END loop over items --#\n",
    "\n",
    "#-- END check to see if anything in list. --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### false negatives (FN)\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize\n",
    "output_string = \"\\n========================================\\nFalse Negatives (FN):\"\n",
    "print( output_string )\n",
    "line_list.append( output_string )\n",
    "\n",
    "# declare variables\n",
    "pub_false_negative_array = None\n",
    "pub_false_negative_mean = None\n",
    "pub_fn_pub_id_list = []\n",
    "pub_fn_count_list = []\n",
    "item_index = None\n",
    "current_count = None\n",
    "output_string = None\n",
    "zipped_fn_lists = None\n",
    "fn_row = None\n",
    "\n",
    "# convert false negative list to a numpy array and get the mean\n",
    "pub_false_negative_array = numpy.array( pub_false_negative_list )\n",
    "pub_false_negative_mean = numpy.mean( pub_false_negative_array )\n",
    "\n",
    "# loop over items, flag any that are over mean\n",
    "item_index = -1\n",
    "for current_count in pub_false_negative_list:\n",
    "    \n",
    "    # increment index\n",
    "    item_index += 1\n",
    "\n",
    "    # get publication ID\n",
    "    publication_id = pub_publication_id_list[ item_index ]\n",
    "    \n",
    "    # is count greater than mean?\n",
    "    if ( current_count > pub_false_negative_mean ):\n",
    "        \n",
    "        # add to list\n",
    "        pub_fn_pub_id_list.append( publication_id )\n",
    "        pub_fn_count_list.append( current_count )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if ( debug_flag == True ):\n",
    "            print( \"- pub {} FN {} <= mean ( {} )\".format( publication_id, current_count, pub_false_negative_mean ) )\n",
    "        #-- END debug --#\n",
    "        \n",
    "    #-- END loop over false negatives. --#\n",
    "    \n",
    "#-- END loop over publications --#\n",
    "\n",
    "# zip up the two lists (one list of pairs of values, rather than two lists).\n",
    "\n",
    "# ID order\n",
    "zipped_fn_lists = list( zip( pub_fn_count_list, pub_fn_pub_id_list ) )\n",
    "\n",
    "# convert to ordered by count, then ID, largest to smallest.\n",
    "zipped_fn_lists.sort( reverse = True )\n",
    "\n",
    "# anything in the list?\n",
    "fn_count = len( zipped_fn_lists )\n",
    "if( fn_count > 0 ):\n",
    "\n",
    "    output_string = \"\\n==> {} False Negatives (FN) above mean ( {} ):\".format( fn_count, pub_false_negative_mean )\n",
    "    print( output_string )\n",
    "    line_list.append( output_string )\n",
    "\n",
    "    # output for review\n",
    "    for fn_row in zipped_fn_lists:\n",
    "\n",
    "        # summarize\n",
    "        output_string = \"- pub {} FN {} > mean ( {} )\".format( fn_row[ 1 ], fn_row[ 0 ], pub_false_negative_mean )\n",
    "        print( output_string )\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END loop over items --#\n",
    "    \n",
    "#-- END check to see if anything in list. --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output all publication-citation pairs\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# output all the full lists\n",
    "current_index = -1\n",
    "for item in publication_id_per_citation_list:\n",
    "    \n",
    "    # increment index\n",
    "    current_index += 1\n",
    "    \n",
    "    # get current values.\n",
    "    baseline_value = baseline_list[ current_index ]\n",
    "    derived_raw_value = derived_raw_list[ current_index ]\n",
    "    derived_binary_value = derived_binary_list[ current_index ]\n",
    "    pub_id_value = publication_id_per_citation_list[ current_index ]\n",
    "    data_set_id_value = data_set_id_per_citation_list[ current_index ]\n",
    "    \n",
    "    print( \"{}: pub ID {} - data set ID {} - baseline {} - binary {} - raw {}\".format( current_index, pub_id_value, data_set_id_value, baseline_value, derived_binary_value, derived_raw_value ) )\n",
    "    \n",
    "#-- END loop over full lists. --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph precision and recall at n\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set new precision_recall_graph_path\n",
    "precision_recall_graph_path = \"{}/{}precision_recall_graph-cited{}.pdf\".format( output_folder_path, file_name_prefix, file_name_suffix )\n",
    "\n",
    "# declare variables\n",
    "plot_details = None\n",
    "\n",
    "# output to file?\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # output figure to file\n",
    "    plot_details = plot_precision_recall_n( baseline_list, derived_raw_list, \"evaluation\", output_path_IN = precision_recall_graph_path )\n",
    "\n",
    "else:\n",
    "    \n",
    "    # just output to standard out (as is possible)\n",
    "    plot_details = plot_precision_recall_n( baseline_list, derived_raw_list, \"evaluation\" )\n",
    "\n",
    "#-- END check to see if output graph to file --#\n",
    "\n",
    "# DEBUG?\n",
    "if ( debug_flag == True ):\n",
    "\n",
    "    # summarize\n",
    "    output_string = \"- plot details: {}\".format( plot_details )\n",
    "    print( output_string )\n",
    "    #line_list.append( output_string )\n",
    "\n",
    "#-- END DEBUG --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output results to file\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set results file path:\n",
    "results_file_path = \"{}/{}evaluation_results-cited{}.txt\".format( output_folder_path, file_name_prefix, file_name_suffix )\n",
    "\n",
    "# declare variables\n",
    "results_file = None\n",
    "line_list_string = None\n",
    "\n",
    "# do we output to file?\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    if ( debug_flag == True ):\n",
    "        print( line_list )\n",
    "    #-- END check to see if debug --#\n",
    "    \n",
    "    # yes.  open output file.\n",
    "    with open( results_file_path, mode = \"w\" ) as results_file:\n",
    "\n",
    "        # join line list with \"/n\", then write.\n",
    "        line_list_string = \"\\n\".join( line_list )\n",
    "        results_file.write( line_list_string )\n",
    "\n",
    "    #-- END with...as --#\n",
    "    \n",
    "    print( \"results output to {}\".format( results_file_path ) )\n",
    "    \n",
    "#-- END check to see if we output to file --#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rich_context (Python 3)",
   "language": "python",
   "name": "rich_context"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
