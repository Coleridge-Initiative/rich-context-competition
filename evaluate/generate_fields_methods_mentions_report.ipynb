{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Load-all-submissions-to-dictionaries\" data-toc-modified-id=\"Load-all-submissions-to-dictionaries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Load all submissions to dictionaries</a></span></li><li><span><a href=\"#Select-fixed-number-of-articles,-baseed-on-10-that-appear-in-all\" data-toc-modified-id=\"Select-fixed-number-of-articles,-baseed-on-10-that-appear-in-all-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Select fixed number of articles, baseed on 10 that appear in all</a></span></li><li><span><a href=\"#Get-all-across-submissions\" data-toc-modified-id=\"Get-all-across-submissions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Get all across submissions</a></span></li><li><span><a href=\"#Write-out-to-file---All-counts-across-CSV\" data-toc-modified-id=\"Write-out-to-file---All-counts-across-CSV-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Write out to file - All counts across CSV</a></span></li><li><span><a href=\"#Write-out-to-file---By-publication-groupings\" data-toc-modified-id=\"Write-out-to-file---By-publication-groupings-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Write out to file - By publication groupings</a></span></li><li><span><a href=\"#Write-out-to-file---Sampled-publications\" data-toc-modified-id=\"Write-out-to-file---Sampled-publications-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Write out to file - Sampled publications</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:50.631116Z",
     "start_time": "2018-12-06T19:54:50.625939Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "import csv\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:50.643556Z",
     "start_time": "2018-12-06T19:54:50.632902Z"
    }
   },
   "outputs": [],
   "source": [
    "#inputs_dir = './methods_fields_mentions/'\n",
    "\n",
    "TEAM_NAME = \"rcc-06\"\n",
    "\n",
    "\n",
    "# dev fold results folder.\n",
    "inputs_dir = \"/work/evaluate/{}/final/results\".format(TEAM_NAME)\n",
    "project_label = [i for i in inputs_dir.split('/') if 'rcc' in i][0]\n",
    "files_path = '/work/evaluate/data/holdout/data/input/files'\n",
    "\n",
    "FIXED_PUBS = [102, 103, 106, 108, 112, 118, 6234, 6868, 8192, 8193]\n",
    "# ALL_POSSIBLE_PUBS = [int(i.split('.txt')[0]) for i in os.listdir(os.path.join(files_path, 'text'))]\n",
    "# FIXED_TEN_RANDOM = random.sample([int(i.split('.txt')[0]) for i in os.listdir(os.path.join(files_path, 'text'))], 10)\n",
    "# FIXED_RANDOM = [6053, 7089, 8695, 10167, 7270, 8877, 10328, 6328, 8200, 8183]\n",
    "\n",
    "# holdout fold results folder.\n",
    "#inputs_dir = \"/work/evaluate/rcc-14/2018.11.19/results\"\n",
    "# project_label = \"rcc-14\"\n",
    "#files_path = '/work/evaluate/data/input/files'\n",
    "\n",
    "#all_submissions = [ ( f, f ) for f in os.listdir(inputs_dir) if not f.startswith('.')]\n",
    "all_submissions = [ ( inputs_dir, project_label ) ]\n",
    "\n",
    "# configs for output\n",
    "#output_main_dir = './evaluate'\n",
    "output_main_dir = '/work/evaluate/{}/final/evaluate/'.format(TEAM_NAME)\n",
    "\n",
    "TOTAL_NUM_PUBS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:50.659711Z",
     "start_time": "2018-12-06T19:54:50.645219Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to create a path if it does not exist\n",
    "def make_path(path_to_make):\n",
    "    if not os.path.exists(path_to_make):\n",
    "        print(\"Creating path {}\".format(path_to_make))\n",
    "        os.makedirs(path_to_make)\n",
    "    else:\n",
    "        print(\"{} already exists\".format(path_to_make))\n",
    "\n",
    "    return path_to_make\n",
    "\n",
    "\n",
    "def clean_text(s):\n",
    "    '''\n",
    "        Return a cleaned string so they're easier to compare.\n",
    "    '''\n",
    "    s = s.lower().replace('\\n', ' ')\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_all(data_file, input_type):\n",
    "    data = json.loads(open(data_file, 'r').read())\n",
    "    all_found = [clean_text(i[input_type]) for i in data]\n",
    "    return all_found\n",
    "\n",
    "\n",
    "def get_all_by_pub(data_file, input_type):\n",
    "    data = json.loads(open(data_file, 'r').read())\n",
    "\n",
    "    all_found_by_pub = {}\n",
    "    for i in data:\n",
    "        pub_id_int = int(i['publication_id'])\n",
    "        all_found_by_pub[pub_id_int] = []\n",
    "    \n",
    "    for i in data:\n",
    "        pub_id_int = int(i['publication_id'])\n",
    "        all_found_by_pub[pub_id_int].append(clean_text(i[input_type]))\n",
    "   \n",
    "    # sort each list\n",
    "    for k, v in all_found_by_pub.items():\n",
    "        all_found_by_pub[k] = sorted(v)\n",
    "        \n",
    "    return all_found_by_pub\n",
    "\n",
    "\n",
    "def get_random_pubs(data, num):\n",
    "    random_pub_listing = {}\n",
    "    for k, v in data.items():\n",
    "        available_list = list(set(v['by_pubs'].keys()))\n",
    "        random_pub_listing[k] = random.sample(available_list, num) \n",
    "    return random_pub_listing\n",
    "\n",
    "\n",
    "def get_all_common_pubids(data):\n",
    "    '''selects the set intersection of all publications in all submissions for a submissions task'''\n",
    "    # all_pubs_separate = [v['by_pubs'] for k, v in data.items() if v['by_pubs'] is not []]\n",
    "    all_pubs_separate = [v['by_pubs'] for k, v in data.items() if v['by_pubs'] is not []]\n",
    "    common_pubs = list(set(all_pubs_separate[0]).intersection(*all_pubs_separate))\n",
    "    common_pubs_normalized = [int(i) for i in common_pubs]\n",
    "        \n",
    "    return common_pubs_normalized\n",
    "\n",
    "def select_set_of_n_pubs(pub_lists, num):\n",
    "    '''given a set number, look at all list and select num results that occur across all'''\n",
    "    pub_lists = [l for l in pub_lists if len(l) > 0]\n",
    "    top_num = list(set(pub_lists[0]).intersection(*pub_lists))\n",
    "    return top_num[:num]\n",
    "\n",
    "def get_all_across_submissions(submission_data):\n",
    "    all_counts_across = {}\n",
    "    per_pub_across = {}\n",
    "    for k, v in submission_data.items():\n",
    "        for i in v['all_counts']:\n",
    "            if i[0] not in all_counts_across.keys():\n",
    "                all_counts_across[i[0]] = i[1]\n",
    "            else:\n",
    "                all_counts_across[i[0]] += i[1]\n",
    "        \n",
    "        for k, v in v['by_pubs'].items():\n",
    "            if k not in per_pub_across.keys():\n",
    "                per_pub_across[k] = v\n",
    "            else:\n",
    "                per_pub_across[k].extend(v)\n",
    "                \n",
    "        for k, v in per_pub_across.items():\n",
    "            per_pub_across[k] = sorted(v)\n",
    "            \n",
    "    all_counts_across = sorted(all_counts_across.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    per_pub_across = sorted(per_pub_across.items(), key=lambda kv: int(kv[0]))\n",
    "    return (all_counts_across, per_pub_across)\n",
    "        \n",
    "    \n",
    "def write_to_csv(path, file, header, data):\n",
    "    filepath = os.path.join(path, file)\n",
    "    with open(filepath, 'w', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(header)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "            \n",
    "def write_to_json(path, file, data):\n",
    "    filepath = os.path.join(path, file)\n",
    "    with open(filepath, 'w', encoding='utf-8') as jsonfile:\n",
    "        json.dump(data, jsonfile, indent=4)\n",
    "        \n",
    "        \n",
    "def write_list_to_txt(path, file, list_of_items):\n",
    "    filepath = os.path.join(path, file)\n",
    "    with open(filepath, 'w', encoding='utf-8') as txtfile:\n",
    "        for item in list_of_items:\n",
    "            txtfile.write(\"{}\\n\".format(item))\n",
    "            \n",
    "def generate_scoring_sheet(file_path, pub_list):\n",
    "    \n",
    "    header = ['', 'fields', 'methods', 'mentions']\n",
    "    row_one = ['submission_overall_frequencies (1-5)', '', '', '']\n",
    "    with open(file_path, 'w') as ofile:\n",
    "        writer = csv.writer(ofile)\n",
    "        writer.writerow(header)\n",
    "        writer.writerow(row_one)\n",
    "        for pub in pub_list:\n",
    "            col_val = \"{} (-1, 0, 1)\".format(pub)\n",
    "            nextrow = [col_val, '', '', '']\n",
    "            writer.writerow(nextrow)\n",
    "        writer.writerow(['', '', '', ''])\n",
    "        writer.writerow(['totals', '', '', ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all submissions to dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.004377Z",
     "start_time": "2018-12-06T19:54:50.661429Z"
    }
   },
   "outputs": [],
   "source": [
    "all_fields = {}\n",
    "all_methods = {}\n",
    "all_mentions = {}\n",
    "for submission_tuple in all_submissions:\n",
    "    \n",
    "    submission_dir = submission_tuple[ 0 ]\n",
    "    project_label = submission_tuple[ 1 ]\n",
    "    \n",
    "    # instantiate place to store all result data by submission type\n",
    "    all_fields[project_label] = {}\n",
    "    all_methods[project_label] = {}\n",
    "    all_mentions[project_label]= {}\n",
    "    \n",
    "    # build the paths to the submission files\n",
    "    fields_data = os.path.join(inputs_dir, submission_dir, 'research_fields.json')\n",
    "    methods_data = os.path.join(inputs_dir, submission_dir, 'methods.json')\n",
    "    try:\n",
    "        mentions_data = os.path.join(inputs_dir, submission_dir, 'data_set_mentions.json')\n",
    "        with open(mentions_data, 'r') as f:\n",
    "            pass\n",
    "    except:\n",
    "        mentions_data = os.path.join(inputs_dir, submission_dir, 'dataset_mentions.json')\n",
    "    #-- try...except --#\n",
    "    \n",
    "    # get all the data for each submission type\n",
    "    curr_fields = get_all(fields_data, 'research_field')\n",
    "    curr_methods = get_all(methods_data, 'method')\n",
    "    curr_mentions = get_all(mentions_data, 'mention')\n",
    "    \n",
    "    # create a count of all results for a submission type\n",
    "    curr_fields_count = dict(Counter(curr_fields))\n",
    "    curr_methods_count = dict(Counter(curr_methods))\n",
    "    curr_mentions_count = dict(Counter(curr_mentions))\n",
    "    \n",
    "    # Sort the list of counts\n",
    "    all_curr_fields_counts = sorted(curr_fields_count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    all_curr_methods_counts = sorted(curr_methods_count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    all_curr_mentions_counts = sorted(curr_mentions_count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    \n",
    "    # store all results for a publication with that publication\n",
    "    curr_fields_by_pub = get_all_by_pub(fields_data, 'research_field')\n",
    "    curr_methods_by_pub = get_all_by_pub(methods_data, 'method')\n",
    "    curr_mentions_by_pub = get_all_by_pub(mentions_data, 'mention')\n",
    "    \n",
    "    # store the counts of everything and by publication list in the dictionary store for each submission\n",
    "    all_fields[project_label]['all_counts'] = all_curr_fields_counts\n",
    "    all_fields[project_label]['by_pubs'] = curr_fields_by_pub\n",
    "    all_methods[project_label]['all_counts'] = all_curr_methods_counts\n",
    "    all_methods[project_label]['by_pubs'] = curr_methods_by_pub\n",
    "    all_mentions[project_label]['all_counts'] = all_curr_mentions_counts\n",
    "    all_mentions[project_label]['by_pubs'] = curr_mentions_by_pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.052145Z",
     "start_time": "2018-12-06T19:54:51.006245Z"
    }
   },
   "outputs": [],
   "source": [
    "all_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.216458Z",
     "start_time": "2018-12-06T19:54:51.053763Z"
    }
   },
   "outputs": [],
   "source": [
    "all_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.269400Z",
     "start_time": "2018-12-06T19:54:51.218184Z"
    }
   },
   "outputs": [],
   "source": [
    "all_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select fixed number of articles, baseed on 10 that appear in all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.276951Z",
     "start_time": "2018-12-06T19:54:51.271065Z"
    }
   },
   "outputs": [],
   "source": [
    "# methods_random = get_random_pubs(all_methods, TOTAL_RANDOM_PUBS)\n",
    "# fields_random = get_random_pubs(all_fields, TOTAL_RANDOM_PUBS)\n",
    "# mentions_random = get_random_pubs(all_mentions, TOTAL_RANDOM_PUBS)\n",
    "methods_pubs = get_all_common_pubids(all_methods)\n",
    "fields_pubs = get_all_common_pubids(all_fields)\n",
    "mentions_pubs = get_all_common_pubids(all_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.297777Z",
     "start_time": "2018-12-06T19:54:51.278600Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(methods_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.316128Z",
     "start_time": "2018-12-06T19:54:51.299317Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(fields_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.334916Z",
     "start_time": "2018-12-06T19:54:51.317521Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(mentions_pubs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.345513Z",
     "start_time": "2018-12-06T19:54:51.336451Z"
    }
   },
   "outputs": [],
   "source": [
    "# pub_selection = select_set_of_n_pubs([methods_pubs, fields_pubs, mentions_pubs],TOTAL_NUM_PUBS)\n",
    "# pub_selection = select_set_of_n_pubs([methods_pubs, fields_pubs, mentions_pubs],TOTAL_NUM_PUBS)\n",
    "pub_selection = FIXED_PUBS\n",
    "# pub_selection = FIXED_RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.359193Z",
     "start_time": "2018-12-06T19:54:51.347101Z"
    }
   },
   "outputs": [],
   "source": [
    "pub_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all across submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.376329Z",
     "start_time": "2018-12-06T19:54:51.360599Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOT USING ACROSS ALL FOR NOW\n",
    "\n",
    "# methods_counts_across, methods_bypub_across = get_all_across_submissions(all_methods)\n",
    "# fields_counts_across, fields_bypub_across = get_all_across_submissions(all_fields)\n",
    "# mentions_counts_across, mentions_bypub_across = get_all_across_submissions(all_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out to file - All counts across CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.389174Z",
     "start_time": "2018-12-06T19:54:51.378042Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOT USING ACROSS ALL FOR NOW\n",
    "\n",
    "# write out the all counts across\n",
    "# mentions_path = make_path(os.path.join(output_main_dir, 'mentions'))\n",
    "# methods_path = make_path(os.path.join(output_main_dir, 'methods'))\n",
    "# fields_path = make_path(os.path.join(output_main_dir, 'fields'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.408775Z",
     "start_time": "2018-12-06T19:54:51.390848Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOT USING ACROSS ALL FOR NOW\n",
    "\n",
    "# write_to_csv(mentions_path, 'mentions_all_counts_across.csv' ,['mention', 'count'], mentions_counts_across)\n",
    "# write_to_csv(methods_path, 'methods_all_counts_across.csv', ['method', 'count'], methods_counts_across)\n",
    "# write_to_csv(fields_path, 'fields_all_counts_across.csv' ,['field', 'count'], fields_counts_across)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out to file - By publication groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.427864Z",
     "start_time": "2018-12-06T19:54:51.410389Z"
    }
   },
   "outputs": [],
   "source": [
    "# NOT USING ACROSS ALL FOR NOW\n",
    "\n",
    "# write_to_json(mentions_path, 'mentions_bypub_across.json', dict(mentions_bypub_across))\n",
    "# write_to_json(methods_path, 'methods_bypub_across.json', dict(methods_bypub_across))\n",
    "# write_to_json(fields_path, 'fields_bypub_across.json', dict(fields_bypub_across))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out to file - Sampled publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.481582Z",
     "start_time": "2018-12-06T19:54:51.429439Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIELDS\n",
    "\n",
    "for k, v in all_fields.items():\n",
    "    #field_output_dir = make_path(os.path.join(fields_path, k))\n",
    "    field_output_dir = make_path(os.path.join(output_main_dir, k))\n",
    "    csvpath = '{}-final_fields_counts.csv'.format(k)\n",
    "    jsonpath = '{}-final_fields_bypub.json'.format(k)\n",
    "    # write the scoring sheet out once\n",
    "    scoring_path = os.path.join(output_main_dir, '{}-final_judges_scoring_sheet.csv'.format(k)) \n",
    "    generate_scoring_sheet(scoring_path, pub_selection)\n",
    "    \n",
    "    write_to_csv(field_output_dir, csvpath, ['field', 'count'], v['all_counts'])\n",
    "    write_to_json(field_output_dir, jsonpath, v['by_pubs'])\n",
    "    \n",
    "    #for pubid in fields_random[k]:\n",
    "    for pubid in pub_selection:\n",
    "        pubdir = make_path(os.path.join(field_output_dir, str(pubid)))\n",
    "        txtfile = '{}_fields.txt'.format(str(pubid))\n",
    "        fields_list = v['by_pubs'].get(pubid, None)\n",
    "        \n",
    "        if fields_list is not None:\n",
    "            write_list_to_txt(pubdir, txtfile, fields_list)\n",
    "            copyfile('{}/text/{}.txt'.format(files_path, str(pubid)), os.path.join(pubdir, '{}.txt'.format(str(pubid))))\n",
    "            copyfile('{}/pdf/{}.pdf'.format(files_path, str(pubid)), os.path.join(pubdir, '{}.pdf'.format(str(pubid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.523645Z",
     "start_time": "2018-12-06T19:54:51.483321Z"
    }
   },
   "outputs": [],
   "source": [
    "# METHODS\n",
    "\n",
    "for k, v in all_methods.items():\n",
    "    # method_output_dir = make_path(os.path.join(methods_path, k))\n",
    "    method_output_dir = make_path(os.path.join(output_main_dir, k))\n",
    "    csvpath = '{}-final_methods_counts.csv'.format(k)\n",
    "    jsonpath = '{}-final_methods_bypub.json'.format(k)\n",
    "    \n",
    "    write_to_csv(method_output_dir, csvpath, ['method', 'count'], v['all_counts'])\n",
    "    write_to_json(method_output_dir, jsonpath, v['by_pubs'])\n",
    "    \n",
    "    #for pubid in methods_random[k]:\n",
    "    for pubid in pub_selection:\n",
    "        pubdir = make_path(os.path.join(method_output_dir, str(pubid)))\n",
    "        txtfile = '{}_methods.txt'.format(str(pubid))\n",
    "        methods_list = v['by_pubs'].get(pubid, None)\n",
    "        if methods_list is not None:\n",
    "            write_list_to_txt(pubdir, txtfile, methods_list)\n",
    "            copyfile('{}/text/{}.txt'.format(files_path, str(pubid)), os.path.join(pubdir, '{}.txt'.format(str(pubid))))\n",
    "            copyfile('{}/pdf/{}.pdf'.format(files_path, str(pubid)), os.path.join(pubdir, '{}.pdf'.format(str(pubid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T19:54:51.612509Z",
     "start_time": "2018-12-06T19:54:51.525329Z"
    }
   },
   "outputs": [],
   "source": [
    "# MENTIONS\n",
    "\n",
    "for k, v in all_mentions.items():\n",
    "    # mention_output_dir = make_path(os.path.join(mentions_path, k))\n",
    "    mention_output_dir = make_path(os.path.join(output_main_dir, k))\n",
    "    csvpath = '{}-final_mentions_counts.csv'.format(k)\n",
    "    jsonpath = '{}-final_mentions_bypub.json'.format(k)\n",
    "    \n",
    "    write_to_csv(mention_output_dir, csvpath, ['mention', 'count'], v['all_counts'])\n",
    "    write_to_json(mention_output_dir, jsonpath, v['by_pubs'])\n",
    "    \n",
    "    # for pubid in mentions_random[k]:\n",
    "    for pubid in pub_selection:\n",
    "        pubdir = make_path(os.path.join(mention_output_dir, str(pubid)))\n",
    "        txtfile = '{}_mentions.txt'.format(str(pubid))\n",
    "        mentions_list = v['by_pubs'].get(pubid, None)\n",
    "        if mentions_list is not None:\n",
    "            write_list_to_txt(pubdir, txtfile, mentions_list)\n",
    "            copyfile('{}/text/{}.txt'.format(files_path, str(pubid)), os.path.join(pubdir, '{}.txt'.format(str(pubid))))\n",
    "            copyfile('{}/pdf/{}.pdf'.format(files_path, str(pubid)), os.path.join(pubdir, '{}.pdf'.format(str(pubid))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
