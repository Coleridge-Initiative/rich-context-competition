{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup---imports\" data-toc-modified-id=\"Setup---imports-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Setup - imports</a></span></li><li><span><a href=\"#Setup---Functions\" data-toc-modified-id=\"Setup---Functions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Setup - Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#function-plot_precision_recall_n\" data-toc-modified-id=\"function-plot_precision_recall_n-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>function plot_precision_recall_n</a></span></li><li><span><a href=\"#function-threshold_at_k\" data-toc-modified-id=\"function-threshold_at_k-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>function threshold_at_k</a></span></li><li><span><a href=\"#function-precision_at_k\" data-toc-modified-id=\"function-precision_at_k-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>function precision_at_k</a></span></li><li><span><a href=\"#function-recall_at_k\" data-toc-modified-id=\"function-recall_at_k-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>function recall_at_k</a></span></li><li><span><a href=\"#function-accuracy_at_k\" data-toc-modified-id=\"function-accuracy_at_k-1.2.5\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>function accuracy_at_k</a></span></li></ul></li><li><span><a href=\"#Setup---output\" data-toc-modified-id=\"Setup---output-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setup - output</a></span></li></ul></li><li><span><a href=\"#class-CitationCodingEvaluation\" data-toc-modified-id=\"class-CitationCodingEvaluation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>class CitationCodingEvaluation</a></span></li><li><span><a href=\"#Load-JSON-files\" data-toc-modified-id=\"Load-JSON-files-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load JSON files</a></span></li><li><span><a href=\"#Process-JSON\" data-toc-modified-id=\"Process-JSON-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Process JSON</a></span></li><li><span><a href=\"#Evaluate\" data-toc-modified-id=\"Evaluate-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluate</a></span><ul class=\"toc-item\"><li><span><a href=\"#precision,-recall,-and-accuracy\" data-toc-modified-id=\"precision,-recall,-and-accuracy-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>precision, recall, and accuracy</a></span></li><li><span><a href=\"#graph-precision-and-recall-at-n\" data-toc-modified-id=\"graph-precision-and-recall-at-n-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>graph precision and recall at n</a></span></li><li><span><a href=\"#output-results-to-file\" data-toc-modified-id=\"output-results-to-file-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>output results to file</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - imports\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import datetime\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - Functions\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function plot_precision_recall_n\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name, output_path_IN = None ):\n",
    "\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    \n",
    "    # imports\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "    # return reference\n",
    "    details_OUT = {}\n",
    "    \n",
    "    # declare variables\n",
    "    y_score = None\n",
    "    precision_curve = None\n",
    "    recall_curve = None\n",
    "    pr_thresholds = None\n",
    "    num_above_thresh = None\n",
    "    pct_above_thresh = None\n",
    "    pct_above_per_thresh = None\n",
    "    current_score = None\n",
    "    above_threshold_list = None\n",
    "    above_threshold_count = -1\n",
    "    \n",
    "    # store the raw scores in y_score\n",
    "    y_score = y_prob\n",
    "    \n",
    "    # calculate precision-recall curve\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html\n",
    "    # Returns:\n",
    "    # - precision_curve - Precison values such that element i is the precision of predictions where cutoff is score >= thresholds[ i ] and the last element is 1.\n",
    "    # - recall_curve - Recall values such that element i is the recall of predictions where cutoff is score >= thresholds[ i ] and the last element is 0.\n",
    "    # - pr_thresholds - Increasing thresholds on the decision function used to decide 1 or 0, used to calculate precision and recall (looks like it is the set of unique values in the predicted value set).\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve( y_true, y_score )\n",
    "    \n",
    "    # get all but the last precision score (1).\n",
    "    precision_curve = precision_curve[ : -1 ]\n",
    "    # print( \"precision_curve: {}\".format( precision_curve ) )\n",
    "    \n",
    "    # get all but the last recall score (0).\n",
    "    recall_curve = recall_curve[ : -1 ]\n",
    "    # print( \"recall_curve: {}\".format( recall_curve ) )\n",
    "    \n",
    "    # store details\n",
    "    details_OUT[ \"precision\" ] = precision_curve\n",
    "    details_OUT[ \"recall\" ] = recall_curve\n",
    "    details_OUT[ \"threshold\" ] = pr_thresholds\n",
    "    \n",
    "    # init loop over thresholds\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    \n",
    "    # loop over thresholds\n",
    "    for value in pr_thresholds:\n",
    "        \n",
    "        # at each threshold, calculate the percent of rows above the threshold.\n",
    "        above_threshold_list = []\n",
    "        above_threshold_count = -1\n",
    "        for current_score in y_score:\n",
    "            \n",
    "            # is it at or above threshold?\n",
    "            if ( current_score >= value ):\n",
    "                \n",
    "                # it is either at or above threshold - add to list.\n",
    "                above_threshold_list.append( current_score )\n",
    "                \n",
    "            #-- END check to see if at or above threshold? --#\n",
    "                \n",
    "        #-- END loop over scores. --#\n",
    "\n",
    "        # how many above threshold?\n",
    "        #num_above_thresh = len(y_score[y_score>=value])\n",
    "        above_threshold_count = len( above_threshold_list )\n",
    "        num_above_thresh = above_threshold_count\n",
    "        \n",
    "        # percent above threshold\n",
    "        pct_above_thresh = num_above_thresh / float( number_scored )\n",
    "        \n",
    "        # add to list.\n",
    "        pct_above_per_thresh.append( pct_above_thresh )\n",
    "        \n",
    "    #-- END loop over thresholds --#\n",
    "\n",
    "    details_OUT[ \"percent_above\" ] = pct_above_per_thresh\n",
    "    \n",
    "    # convert to numpy array\n",
    "    pct_above_per_thresh = numpy.array(pct_above_per_thresh)\n",
    "\n",
    "    # init matplotlib\n",
    "    matplotlib.pyplot.clf()\n",
    "    fig, ax1 = matplotlib.pyplot.subplots()\n",
    "    \n",
    "    # plot precision line\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    \n",
    "    # plot recall line\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    # finish off graph\n",
    "    name = model_name\n",
    "    matplotlib.pyplot.title(name)\n",
    "    \n",
    "    # is there an output path?\n",
    "    if ( ( output_path_IN is not None ) and ( output_path_IN != \"\" ) ):\n",
    "    \n",
    "        # save the figure to file.\n",
    "        matplotlib.pyplot.savefig( output_path_IN )\n",
    "    \n",
    "    #-- END check to see if we output to disk. --#\n",
    "    \n",
    "    matplotlib.pyplot.show()\n",
    "\n",
    "    # clear plot.\n",
    "    matplotlib.pyplot.clf()\n",
    "    \n",
    "    return details_OUT\n",
    "    \n",
    "#-- END function plot_precision_recall_n() --#\n",
    "\n",
    "print( \"function plot_precision_recall_n() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function threshold_at_k\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_at_k( y_scores, k ):\n",
    "    \n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    value_list = None\n",
    "    threshold_index = -1\n",
    "    \n",
    "    # sort values\n",
    "    value_list = np.sort( y_scores )\n",
    "    \n",
    "    # reverse order of list\n",
    "    value_list = value_list[ : : -1 ]\n",
    "    \n",
    "    # calculate index of value that is k% of the way through the sorted distribution of scores\n",
    "    threshold_index = int( k * len( y_scores ) )\n",
    "    \n",
    "    # get value that is k% of the way through the sorted distribution of scores\n",
    "    value_OUT = value_list[ threshold_index ]\n",
    "    \n",
    "    print( \"Threshold: {}\".format( value_OUT ) )\n",
    "    \n",
    "    return value_OUT\n",
    "\n",
    "#-- END function threshold_at_k() --#\n",
    "\n",
    "print( \"function threshold_at_k() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function precision_at_k\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k( y_true, y_scores, k ):\n",
    "    \n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    threshold = None\n",
    "    \n",
    "    # get threshold index\n",
    "    threshold = threshold_at_k( y_scores, k )\n",
    "    \n",
    "    # use threshold to generate predicted scores\n",
    "    y_pred = np.asarray( [ 1 if i >= threshold else 0 for i in y_scores ] )\n",
    "    \n",
    "    # calculate precision\n",
    "    value_OUT = precision_score( y_true, y_pred )\n",
    "    \n",
    "    return value_OUT\n",
    "\n",
    "#-- END function precision_at_k() --#\n",
    "\n",
    "print( \"function precision_at_k() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function recall_at_k\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k( y_true, y_scores, k ):\n",
    "    \n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    threshold = None\n",
    "    \n",
    "    # get threshold index\n",
    "    threshold = threshold_at_k( y_scores, k )\n",
    "    \n",
    "    # use threshold to generate predicted scores\n",
    "    y_pred = np.asarray( [ 1 if i >= threshold else 0 for i in y_scores ] )\n",
    "    \n",
    "    # calculate recall\n",
    "    value_OUT = recall_score( y_true, y_pred )\n",
    "    \n",
    "    return value_OUT\n",
    "\n",
    "#-- END function recall_at_k() --#\n",
    "\n",
    "print( \"function recall_at_k() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function accuracy_at_k\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_at_k( y_true, y_scores, k ):\n",
    "    \n",
    "    # return reference\n",
    "    value_OUT = None\n",
    "    \n",
    "    # declare variables\n",
    "    threshold = None\n",
    "    \n",
    "    # get threshold index\n",
    "    threshold = threshold_at_k( y_scores, k )\n",
    "    \n",
    "    # use threshold to generate predicted scores\n",
    "    y_pred = np.asarray( [ 1 if i >= threshold else 0 for i in y_scores ] )\n",
    "    \n",
    "    # calculate accuracy\n",
    "    value_OUT = accuracy_score( y_true, y_pred )\n",
    "    \n",
    "    return value_OUT\n",
    "\n",
    "#-- END function accuracy_at_k() --#\n",
    "\n",
    "print( \"function accuracy_at_k() defined at {}\".format( datetime.datetime.now() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup - output\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_to_file flag\n",
    "output_to_file = True\n",
    "line_list = None\n",
    "output_string = None\n",
    "#output_folder_path = \"/data/output\"\n",
    "output_folder_path = \".\"\n",
    "results_file_path = \"{}/evaluation_results.txt\".format( output_folder_path )\n",
    "precision_recall_graph_path = \"{}/precision_recall_graph.pdf\".format( output_folder_path )\n",
    "\n",
    "# if we are outputting to file, start line list.\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # put a list in line_list\n",
    "    line_list = []\n",
    "    \n",
    "#-- END init line list --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class CitationCodingEvaluation\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from citation_coding_evaluation import CitationCodingEvaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load JSON files\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "baseline_json_path = \"./data_set_citations.json\"\n",
    "derived_prefix = \"\"\n",
    "# set to \"..\" for running against in-repo code development\n",
    "#derived_prefix = \"..\"\n",
    "derived_json_path = \"{}/data/output/data_set_citations.json\".format( derived_prefix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the baseline JSON\n",
    "baseline_json_file = None\n",
    "baseline_json = None\n",
    "\n",
    "# if output...\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( \"Reading baseline file: {}\".format( baseline_json_path ) )\n",
    "    \n",
    "#-- END if output... --#\n",
    "\n",
    "# baseline\n",
    "with open( baseline_json_path ) as baseline_json_file:\n",
    "\n",
    "    # load the JSON from the file.\n",
    "    baseline_json = json.load( baseline_json_file )\n",
    "\n",
    "#-- END with...as --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the derived JSON\n",
    "derived_json_file = None\n",
    "derived_json = None\n",
    "\n",
    "# if output...\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( \"Reading derived file: {}\".format( derived_json_path ) )\n",
    "    \n",
    "#-- END if output... --#\n",
    "\n",
    "# baseline\n",
    "with open( derived_json_path ) as derived_json_file:\n",
    "\n",
    "    # load the JSON from the file.\n",
    "    derived_json = json.load( derived_json_file )\n",
    "\n",
    "#-- END with...as --#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "baseline_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "derived_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process JSON\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init class to handle evaluation\n",
    "coding_evaluator = CitationCodingEvaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process baseline JSON\n",
    "result_type = CitationCodingEvaluation.RESULT_TYPE_BASELINE\n",
    "citation_json = baseline_json\n",
    "status = coding_evaluator.process_citation_json( citation_json, result_type )\n",
    "\n",
    "# output\n",
    "output_string = \"Processing status for {} (None = Success!): \\\"{}\\\"\".format( result_type, status )\n",
    "print( output_string )\n",
    "\n",
    "# if output...\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "    \n",
    "#-- END if output... --#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process derived JSON\n",
    "result_type = CitationCodingEvaluation.RESULT_TYPE_DERIVED\n",
    "citation_json = derived_json\n",
    "status = coding_evaluator.process_citation_json( citation_json, result_type )\n",
    "\n",
    "# output\n",
    "output_string = \"Processing status for {} (None = Success!): \\\"{}\\\"\".format( result_type, status )\n",
    "print( output_string )\n",
    "\n",
    "# if output...\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # store line for output\n",
    "    line_list.append( output_string )\n",
    "    \n",
    "#-- END if output... --#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists\n",
    "status = coding_evaluator.create_evaluation_lists()\n",
    "baseline_list = coding_evaluator.get_baseline_list()\n",
    "derived_raw_list = coding_evaluator.get_derived_raw_list()\n",
    "derived_binary_list = coding_evaluator.get_derived_binary_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision, recall, and accuracy\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculation methods to include\n",
    "calculation_methods = []\n",
    "calculation_methods.append( \"binary\" )\n",
    "calculation_methods.append( \"macro\" )\n",
    "calculation_methods.append( \"micro\" )\n",
    "calculation_methods.append( \"weighted\" )\n",
    "\n",
    "# ==> basic binary scores\n",
    "if ( \"binary\" in calculation_methods ):\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = metrics.confusion_matrix( baseline_list, derived_binary_list )\n",
    "    print( cm )\n",
    "\n",
    "    # output\n",
    "    output_string = \"Confusion matrix: {}\".format( cm )\n",
    "\n",
    "    # if output...\n",
    "    if ( output_to_file == True ):\n",
    "\n",
    "        # store line for output\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "\n",
    "    # calculate precision, recall, accuracy...\n",
    "\n",
    "    # ==> precision\n",
    "    precision = metrics.precision_score( baseline_list, derived_binary_list )\n",
    "\n",
    "    # output\n",
    "    output_string = \"precision = {}\".format( precision )\n",
    "    print( output_string )\n",
    "\n",
    "    # if output...\n",
    "    if ( output_to_file == True ):\n",
    "\n",
    "        # store line for output\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "\n",
    "    # ==> recall\n",
    "    recall = metrics.recall_score( baseline_list, derived_binary_list )\n",
    "\n",
    "    # output\n",
    "    output_string = \"recall = {}\".format( recall )\n",
    "    print( output_string )\n",
    "\n",
    "    # if output...\n",
    "    if ( output_to_file == True ):\n",
    "\n",
    "        # store line for output\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "\n",
    "    # ==> accuracy\n",
    "    accuracy = metrics.accuracy_score( baseline_list, derived_binary_list )\n",
    "\n",
    "    # output\n",
    "    output_string = \"accuracy = {}\".format( accuracy )\n",
    "    print( output_string )\n",
    "\n",
    "    # if output...\n",
    "    if ( output_to_file == True ):\n",
    "\n",
    "        # store line for output\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "    \n",
    "    # F-Score\n",
    "    binary_evaluation = metrics.precision_recall_fscore_support( baseline_list, derived_binary_list )\n",
    "    binary_precision = binary_evaluation[ 0 ][ 0 ]\n",
    "    binary_recall = binary_evaluation[ 1 ][ 0 ]\n",
    "    binary_F1 = binary_evaluation[ 2 ][ 0 ]\n",
    "\n",
    "    # output\n",
    "    output_string = \"binary: precision = {}, recall = {}, F1 = {}\".format( binary_precision, binary_recall, binary_F1 )\n",
    "    print( output_string )\n",
    "\n",
    "    # if output...\n",
    "    if ( output_to_file == True ):\n",
    "\n",
    "        # store line for output\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "\n",
    "#-- END binary F-Score --#\n",
    "\n",
    "# ==> macro F-Score\n",
    "if ( \"macro\" in calculation_methods ):\n",
    "\n",
    "    macro_evaluation = metrics.precision_recall_fscore_support( baseline_list, derived_binary_list, average = 'macro' )\n",
    "    macro_precision = macro_evaluation[ 0 ]\n",
    "    macro_recall = macro_evaluation[ 1 ]\n",
    "    macro_F1 = macro_evaluation[ 2 ]\n",
    "\n",
    "    # output\n",
    "    output_string = \"macro-average: precision = {}, recall = {}, F1 = {}\".format( macro_precision, macro_recall, macro_F1 )\n",
    "    print( output_string )\n",
    "\n",
    "    # if output...\n",
    "    if ( output_to_file == True ):\n",
    "\n",
    "        # store line for output\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "    \n",
    "#-- END macro F-Score --#\n",
    "\n",
    "# ==> micro F-Score\n",
    "if ( \"micro\" in calculation_methods ):\n",
    "\n",
    "    micro_evaluation = metrics.precision_recall_fscore_support( baseline_list, derived_binary_list, average = 'micro' )\n",
    "    micro_precision = micro_evaluation[ 0 ]\n",
    "    micro_recall = micro_evaluation[ 1 ]\n",
    "    micro_F1 = micro_evaluation[ 2 ]\n",
    "\n",
    "    # output\n",
    "    output_string = \"micro-average: precision = {}, recall = {}, F1 = {}\".format( micro_precision, micro_recall, micro_F1 )\n",
    "    print( output_string )\n",
    "\n",
    "    # if output...\n",
    "    if ( output_to_file == True ):\n",
    "\n",
    "        # store line for output\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "\n",
    "#-- END micro F-Score --#\n",
    "    \n",
    "# ==> weighted F-Score\n",
    "if ( \"weighted\" in calculation_methods ):\n",
    "\n",
    "    weighted_evaluation = metrics.precision_recall_fscore_support( baseline_list, derived_binary_list, average = 'weighted' )\n",
    "    weighted_precision = weighted_evaluation[ 0 ]\n",
    "    weighted_recall = weighted_evaluation[ 1 ]\n",
    "    weighted_F1 = weighted_evaluation[ 2 ]\n",
    "\n",
    "    # output\n",
    "    output_string = \"weighted-average: precision = {}, recall = {}, F1 = {}\".format( weighted_precision, weighted_recall, weighted_F1 )\n",
    "    print( output_string )\n",
    "\n",
    "    # if output...\n",
    "    if ( output_to_file == True ):\n",
    "\n",
    "        # store line for output\n",
    "        line_list.append( output_string )\n",
    "\n",
    "    #-- END if output... --#\n",
    "\n",
    "#-- END weighted F-Score --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph precision and recall at n\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to file?\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # output figure to file\n",
    "    plot_precision_recall_n( baseline_list, derived_raw_list, \"evaluation\", output_path_IN = precision_recall_graph_path )\n",
    "\n",
    "else:\n",
    "    \n",
    "    # just output to standard out (as is possible)\n",
    "    plot_precision_recall_n( baseline_list, derived_raw_list, \"evaluation\" )\n",
    "\n",
    "#-- END check to see if output graph to file --#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output results to file\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "results_file = None\n",
    "line_list_string = None\n",
    "\n",
    "# do we output to file?\n",
    "if ( output_to_file == True ):\n",
    "    \n",
    "    # yes.  open output file.\n",
    "    with open( results_file_path, mode = \"w\" ) as results_file:\n",
    "\n",
    "        # join line list with \"/n\", then write.\n",
    "        line_list_string = \"\\n\".join( line_list )\n",
    "        results_file.write( line_list_string )\n",
    "\n",
    "    #-- END with...as --#    \n",
    "    \n",
    "#-- END check to see if we output to file --#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
